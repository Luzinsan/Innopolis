{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PZKrlwajuGZ-",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Week 11: Ensemble Learning\n",
    "\n",
    "```\n",
    "- Machine Learning, Innopolis University (Fall semester 2024)\n",
    "- Instructors: Adil Khan & Gcinizwe Dlamini\n",
    "```\n",
    "<hr>\n",
    "\n",
    "\n",
    "```\n",
    "In this lab, you will practice Ensemble learning\n",
    "\n",
    "Lab Plan : Ensemble learning\n",
    "    1. Bagging\n",
    "    2. Random Forests\n",
    "    3. AdaBoost\n",
    "```\n",
    "\n",
    "<hr>\n",
    "\n",
    "\n",
    "## Recap\n",
    "* Why ensemble learning?\n",
    "* How does it help?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4G2NXb2yznzd",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Ensemble learning\n",
    "We will explore ensemble learning on the example of decision trees - we will see how ensembles can improve classification accuracy.\n",
    "\n",
    "Let's start from uploading MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 290
    },
    "id": "9sI82NDtzoSP",
    "outputId": "b775a1d8-1d58-4299-b66a-c57c8f2296ad",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "digits = load_digits()\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=0)\n",
    "\n",
    "\n",
    "plt.figure(1, figsize=(3, 3))\n",
    "plt.imshow(X[1].reshape((8,8)), cmap=\"gray\")\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.title(f\"label is {y[1]}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TcJSouftKwUC",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Single decision tree\n",
    "\n",
    "First, we train a single decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aqrQbHVDKw9F",
    "outputId": "09b6d982-ab32-4f18-de93-1a2a2b34f6fb",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single tree accuracy: 0.8501683501683501\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "np.random.seed(42)\n",
    "\n",
    "tree = DecisionTreeClassifier()\n",
    "tree.fit(X_train, y_train)\n",
    "pred = tree.predict(X_test)\n",
    "tree_score = accuracy_score(y_test, pred)\n",
    "print(\"Single tree accuracy:\", tree_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qgAj4l5GLKuG",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Note the accuracy - it is around **0.85**.\n",
    "\n",
    "## 1. Bagging\n",
    "\n",
    "\n",
    "What is decreased by bagging? Variance or bias? How? <span style=\"color:blue\"> Averaging reduces variance.\n",
    "$$Var\\left(\\frac{1}{n}\\sum_{n=1}^{n}x_i\\right) = \\frac{1}{n^2}var\\left(\\sum_{n=1}^{n}x_i\\right) = \\frac{1}{n^2}\\left(\\sum_{n=1}^{n}var\\left(x_i\\right)\\right) = \\frac{1}{n^2} \\sum_{n=1}^{n} \\sigma^2 = \\frac{1}{n^2} * n * \\sigma^2 = \\frac{\\sigma^2}{n}$$\n",
    "    \n",
    "\n",
    "\n",
    "Now let's improve it a bit by the means of bagging. We train a hundred of independent classifiers and make a prediction by majority voting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0OByvaBo6Etn",
    "outputId": "74ccaa33-b956-415f-aea1-4e35b69bcc2c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging accuracy: 0.9494949494949495\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import mode\n",
    "np.random.seed(42) \n",
    "\n",
    "n_trees = 100\n",
    "\n",
    "classifiers = []\n",
    "for i in range(n_trees):\n",
    "    # train a new classifier and append it to the list\n",
    "    indx = np.random.randint(0,len(y_train),len(y_train))\n",
    "    tree = DecisionTreeClassifier()\n",
    "    tree.fit(X_train[indx], y_train[indx])\n",
    "    classifiers.append(tree)\n",
    "\n",
    "# here we will store predictions for all samples and all base classifiers\n",
    "base_pred = np.zeros((X_test.shape[0], n_trees), dtype=\"int\")\n",
    "for i in range(n_trees):\n",
    "    # obtain the predictions from each tree\n",
    "    base_pred[:,i] = classifiers[i].predict(X_test)\n",
    "\n",
    "# aggregate predictions by majority voting\n",
    "pred = mode(base_pred, axis=1)[0].ravel()\n",
    "acc = accuracy_score(y_test, pred)\n",
    "print(\"Bagging accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PZPDvGFT7217"
   },
   "source": [
    "### No Bootstrapping example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xWQTCYrGLLMM",
    "outputId": "1ba039ef-2b2b-4127-97b1-c26458fe60f3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging accuracy: 0.877104377104377\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import mode\n",
    "np.random.seed(42)\n",
    "\n",
    "n_trees = 100\n",
    "\n",
    "classifiers = []\n",
    "for i in range(n_trees):\n",
    "    # train a new classifier and append it to the list\n",
    "    tree = DecisionTreeClassifier()\n",
    "    tree.fit(X_train, y_train)\n",
    "    classifiers.append(tree)\n",
    "\n",
    "# here we will store predictions for all samples and all base classifiers\n",
    "base_pred = np.zeros((X_test.shape[0], n_trees), dtype=\"int\")\n",
    "for i in range(n_trees):\n",
    "    # obtain the predictions from each tree\n",
    "    base_pred[:,i] = classifiers[i].predict(X_test)\n",
    "\n",
    "# aggregate predictions by majority voting\n",
    "pred = mode(base_pred, axis=1)[0].ravel()\n",
    "acc = accuracy_score(y_test, pred)\n",
    "print(\"Bagging accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ODOPvJJ2bKPh",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now the accuracy grew up to **0.88**. You can see that our classifiers return very similar results. By the way, why the base classifiers are not identical at all? <span style=\"color:blue\"> That is the case, if the improvement of the criterion is identical for several splits and one split has to be selected at random.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yUM5zWhX6Etn",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2. Random forest\n",
    "\n",
    "Compared to simple bagging we've just implemented, random forest can show better results because base classifiers are much less correlated.\n",
    "\n",
    "<span style=\"color:red\">TODO: At first, implement bootstrap sampling (use numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C9d1ElFpE48c",
    "outputId": "80a201c4-3e46-4c00-8cfe-7691fe87b2eb",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0,  1,  2],\n",
       "        [ 9, 10, 11],\n",
       "        [ 3,  4,  5],\n",
       "        [ 0,  1,  2]]),\n",
       " array([0, 3, 1, 0]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def bootstrap(X, y, n_features):\n",
    "    # TODO: generate bootstrap indices and return data according to them\n",
    "    idx = np.random.randint(0,X.shape[0], X.shape[0])\n",
    "    return X[idx],  y[idx]\n",
    "\n",
    "\n",
    "# this is a test, will work if you are using np.random.randint() for indices generation\n",
    "np.random.seed(0)\n",
    "a = np.array(range(12)).reshape(4,3)\n",
    "b = np.array(range(4))\n",
    "bootstrap(a, b, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NxpCck94Y73A",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "You should get\n",
    "\n",
    "(array([[ 0,  1,  2], <br>\n",
    "&emsp;&emsp;&emsp;[ 9, 10, 11], <br>\n",
    "&emsp;&emsp;&emsp;[ 3,  4,  5], <br>\n",
    "&emsp;&emsp;&emsp;[ 0,  1,  2]]), <br>\n",
    "array([0, 3, 1, 0]))\n",
    "       \n",
    "Now let's build a set of decision trees, each of them is trained on a bootstrap sampling from X and $\\sqrt d$ features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HxIhmI_H5jnI",
    "outputId": "9093f84d-68c3-45bf-9600-571714265b82",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest accuracy: 0.9814814814814815\n"
     ]
    }
   ],
   "source": [
    "classifiers = []\n",
    "for i in range(n_trees):\n",
    "    # train a new tree on sqrt(n_features) and bootstrapped data, append it to the list\n",
    "    base = DecisionTreeClassifier(max_features=\"sqrt\")\n",
    "    bs_X, bs_y = bootstrap(X_train, y_train, 50)\n",
    "    base.fit(bs_X, bs_y)\n",
    "    classifiers.append(base)\n",
    "\n",
    "base_pred = np.zeros((n_trees, X_test.shape[0]), dtype=\"int\")\n",
    "for i in range(n_trees):\n",
    "    base_pred[i,:] = classifiers[i].predict(X_test)\n",
    "\n",
    "pred = mode(base_pred, axis=0)[0].ravel()\n",
    "acc = accuracy_score(y_test, pred)\n",
    "print(\"Random forest accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ObGBbQfrE5jM",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "And now we got **0.97** accuracy, which is a significant improvement! Now you can see why it is so important to have diverse classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qp4WWdYezpHy",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3. Boosting\n",
    "\n",
    "How does boosting work? <span style=\"color:blue\"> Models are built sequentially: each model is built using information from previously built models. Boosting does not involve bootstrap sampling; instead each tree is fit on a modified version of the original data set.\n",
    "\n",
    "For simplicity let's make a binary classification problem out of the original problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "JBbO7p-aNk69"
   },
   "outputs": [],
   "source": [
    "y_train_b = (y_train == 2 ) * 2 - 1\n",
    "y_test_b = (y_test == 2 ) * 2 - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "talvfivTQitE"
   },
   "source": [
    "Now let's train a boosting model.\n",
    "\n",
    "We will have sample weights and tree weights. Initially all sample weights are equal. After that we will increase weight for complicated samples.\n",
    "\n",
    "Tree weight $w$ is computed using weighted error or $1 - accuracy$\n",
    "\n",
    "$w_t = \\frac12 log\\left(\\frac{1-weighted\\_error_t}{weighted\\_error_t}\\right)$ for each base classifier.\n",
    "\n",
    "For correct samples weights will be decreased $e^w$ times, and for incorrect classified samples increased  $e^w$ times. After this changes we normalize weights.\n",
    "\n",
    "\n",
    "## 3.1 Boosting from Scratch\n",
    "\n",
    "Tasks:\n",
    "1. initialize sample weights\n",
    "1. calculate tree weight\n",
    "1. update sample weights\n",
    "1. normalize the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "mVEBRi4pEwte",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_trees = 10\n",
    "tree_weights = np.zeros(n_trees)\n",
    "classifiers = []\n",
    "train_samples = X_train.shape[0]\n",
    "\n",
    "# TODO: initialize sample weights\n",
    "sample_weights = np.ones(train_samples) / train_samples\n",
    "\n",
    "\n",
    "\n",
    "for i in range(n_trees):\n",
    "    clf = DecisionTreeClassifier(min_samples_leaf=3)\n",
    "    clf.fit(X_train, y_train_b, sample_weight=sample_weights)\n",
    "    pred = clf.predict(X_train)\n",
    "    acc = accuracy_score(y_train_b, pred, sample_weight=sample_weights)\n",
    "\n",
    "    # TODO: caclulate tree weight\n",
    "    # weighted_error = 1 - acc\n",
    "    w = 0.5 * np.log(acc/(1 - acc)) \n",
    "    tree_weights[i] = w\n",
    "    classifiers.append(clf)\n",
    "\n",
    "    # TODO: update sample weights\n",
    "    for j in range(train_samples):\n",
    "        if pred[j] != y_train_b[j]:\n",
    "            sample_weights[j] = sample_weights[j] * np.exp(w)\n",
    "        else:\n",
    "            sample_weights[j] = sample_weights[j] * np.exp(-w)\n",
    "\n",
    "\n",
    "    # TODO: normalize the weights\n",
    "    sample_weights = sample_weights / np.sum(sample_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YJ43LIorSXbs"
   },
   "source": [
    "Use trees voting to calculate final predictions. Since we have a binary classification, the prediction will be calculated as follows:\n",
    "\n",
    "$\\hat{y} = sign\\left(\\sum_{t=1}^{T}(w_t f_t(x))\\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PpJKjCDdzpmt",
    "outputId": "025b198d-09d9-4607-fb97-ff8c7bac422c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boosting accuracy: 0.9831649831649831\n"
     ]
    }
   ],
   "source": [
    "n_test = X_test.shape[0]\n",
    "\n",
    "pred = np.zeros(n_test)\n",
    "# caclulate predictions\n",
    "for t in range(n_trees):\n",
    "    pred += classifiers[t].predict(X_test) * tree_weights[t]\n",
    "for i in range(n_test):\n",
    "    pred[i] = 1 if pred[i] > 0 else -1\n",
    "\n",
    "\n",
    "acc = accuracy_score(y_test_b, pred)\n",
    "print(\"Boosting accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E7mv1TfwSahW",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The resulting accuracy is **0.97**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_afJfuRC6Etp",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Self practice task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3O2MkWOf6Etp",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

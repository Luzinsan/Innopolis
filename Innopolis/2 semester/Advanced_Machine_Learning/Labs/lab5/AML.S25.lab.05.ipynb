{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQWLygmO75Mq"
      },
      "source": [
        "# Lab 5 : More on Recurrent neural networks (LSTM)\n",
        "```\n",
        "- [S25] Advanced Machine Learning, Innopolis University\n",
        "- Teaching Assistant: Gcinizwe Dlamini\n",
        "```\n",
        "<hr>\n",
        "\n",
        "\n",
        "```\n",
        "Lab Plan\n",
        "1. LSTM basics\n",
        "2. Application of LSTM\n",
        "3. Self practice tasks\n",
        "```\n",
        "\n",
        "<hr>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcdKLBsZ8mOD"
      },
      "source": [
        "## 0. Recap\n",
        "\n",
        "![](http://karpathy.github.io/assets/rnn/diags.jpeg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CEDP2Z38xEA"
      },
      "source": [
        "## Sample Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HjJf-itv7vrX",
        "outputId": "cc9b3bdf-3980-43c3-ea75-078779562e4e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 4, 3])"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "simple_sequence = torch.Tensor([[0.3,1.9,4.5],[0.4,0.1,0.23],[0.7,0.91,0.43], [0.34,0.01,0.002]])\n",
        "simple_sequence = simple_sequence.unsqueeze(0)\n",
        "simple_sequence.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5m4x8BY86Wh"
      },
      "source": [
        "## 1. LSTM basics\n",
        "\n",
        "The `simple_sequence` variable represents a sequence of length 4, where each element (time-stamp) is represented by a feature vector of length 3. LSTM calculations are defined as:\n",
        "\n",
        "![](https://media.licdn.com/dms/image/v2/C5612AQH5Im8XrvLmYQ/article-cover_image-shrink_600_2000/article-cover_image-shrink_600_2000/0/1564974698831?e=2147483647&v=beta&t=4sP9wrqZVaKsUt8NLXwuN4hfYc0m8RKI3a5g_jUW2xc)\n",
        "\n",
        "\n",
        "$$i_{t} = \\sigma\\left(W_{ii}x_t + b_{ii} + W_{hi}h_{t-1} + b_{hi} \\right)$$\n",
        "$$f_t = \\sigma \\left( W_{if}x_t + b_{if} + W_{hf}h_{t-1} + b_{hf} \\right)$$\n",
        "$$g_t = tanh(W_{ig}x_t + b_{ig} + W_{hg}h_{t-1} + b_{hg})$$\n",
        "$$o_t = \\sigma \\left( W_{io}x_t + b_{io} + W_{ho}h_{t-1} + h_{ho}\\right)$$\n",
        "$$c_t = f_t \\odot c_{t-1} + i_t \\odot g_t$$\n",
        "$$h_t = o_t \\odot tanh(c_t)$$\n",
        "\n",
        "where $h_t$ represents the hidden state at time $t$; $c_t$ cell cell state at time $t$, $x_t$ is the input at time $t$, $h_{t-1}$ is the hidden state of the layer at time $t-1$ or the initial hidden state at time 0, and $i_t$, $f_t$, $g_t$, $o_t$ are the input, forget, cell, and output gates, respectively.\n",
        "\n",
        " <br>\n",
        "Lets see whats inside Pytorch and compare with our theory\n",
        "\n",
        "**Note:** For simplicity, the bias is set to zeros and weights set to ones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ay97aV_f9FMb",
        "outputId": "7264ecc6-21f6-4607-a58b-a73a7c1cf6db"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "OrderedDict([('weight_ih_l0',\n",
              "              tensor([[-0.9475, -0.6130, -0.1291],\n",
              "                      [-0.4107,  1.3931, -0.0984],\n",
              "                      [ 1.6791, -0.9381, -0.4899],\n",
              "                      [ 0.2811, -0.2813,  0.4779]])),\n",
              "             ('weight_hh_l0',\n",
              "              tensor([[ 0.8846],\n",
              "                      [-0.4928],\n",
              "                      [ 0.4776],\n",
              "                      [ 0.0807]])),\n",
              "             ('bias_ih_l0', tensor([0., 0., 0., 0.])),\n",
              "             ('bias_hh_l0', tensor([0., 0., 0., 0.]))])"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.manual_seed(20)\n",
        "hidden_size = 1\n",
        "simple_lstm_layer = torch.nn.LSTM(input_size=3, hidden_size=hidden_size, bidirectional=False, num_layers=1, batch_first=True)\n",
        "\n",
        "\n",
        "share_weight = torch.randn(simple_lstm_layer.weight_ih_l0.shape, dtype = torch.float)\n",
        "simple_lstm_layer.weight_ih_l0 = torch.nn.Parameter(share_weight)\n",
        "\n",
        "# bias set to zeros\n",
        "simple_lstm_layer.bias_ih_l0 = torch.nn.Parameter(torch.zeros(simple_lstm_layer.bias_ih_l0.shape))\n",
        "simple_lstm_layer.bias_hh_l0 = torch.nn.Parameter(torch.zeros(simple_lstm_layer.bias_ih_l0.shape))\n",
        "\n",
        "lstm_pytorch_output = simple_lstm_layer(simple_sequence[0][0].unsqueeze(dim=0).unsqueeze(dim=0))\n",
        "simple_lstm_layer.state_dict()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAGk_INpElZD"
      },
      "source": [
        "### Whole sequence output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "SZJiylw80wSH"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[[-0.0976],\n",
              "          [ 0.0470],\n",
              "          [ 0.0490],\n",
              "          [ 0.1372]]], grad_fn=<TransposeBackward0>),\n",
              " tensor([[[0.2679]]], grad_fn=<StackBackward0>),\n",
              " tensor([[[0.1372]]], grad_fn=<StackBackward0>))"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output, (hidden, cell) = simple_lstm_layer(simple_sequence)\n",
        "output, cell, hidden"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gicPct9wED-c"
      },
      "source": [
        "### 1.2 Extract / define the calculation variables (weights \\& bias)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "lEaUqCUY9N_x"
      },
      "outputs": [],
      "source": [
        "W_ii, W_if, W_ig, W_io = simple_lstm_layer.weight_ih_l0.split(hidden_size, dim=0)\n",
        "b_ii, b_if, b_ig, b_io = simple_lstm_layer.bias_ih_l0.split(hidden_size, dim=0)\n",
        "\n",
        "W_hi, W_hf, W_hg, W_ho = simple_lstm_layer.weight_hh_l0.split(hidden_size, dim=0)\n",
        "b_hi, b_hf, b_hg, b_ho = simple_lstm_layer.bias_hh_l0.split(hidden_size, dim=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9kRbMM3EZAe"
      },
      "source": [
        "### 2.2 Calculations\n",
        "\n",
        "$i_{t} = \\sigma\\left(W_{ii}x_t + b_{ii} + W_{hi}h_{t-1} + b_{hi} \\right)$ <br>\n",
        "$f_t = \\sigma \\left( W_{if}x_t + b_{if} + W_{hf}h_{t-1} + b_{hf} \\right)$ <br>\n",
        "$g_t = tanh(W_{ig}x_t + b_{ig} + W_{hg}h_{t-1} + b_{hg})$ <br>\n",
        "$o_t = \\sigma \\left( W_{io}x_t + b_{io} + W_{ho}h_{t-1} + h_{ho}\\right)$ <br>\n",
        "$c_t = f_t \\odot c_{t-1} + i_t \\odot g_t$ <br>\n",
        "$h_t = o_t \\odot tanh(c_t)$ <br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_x = simple_sequence[0][0].unsqueeze(0)\n",
        "prev_h = torch.zeros((1, hidden_size))\n",
        "prev_c = torch.zeros((1, hidden_size))\n",
        "\n",
        "i_t = torch.sigmoid(F.linear(input_x, W_ii, b_ii) +  F.linear(W_hi, prev_h, b_hi))\n",
        "f_t = torch.sigmoid(F.linear(input_x, W_if, b_if) +  F.linear(prev_h, W_hf, b_hf))\n",
        "g_t = torch.tanh(F.linear(input_x, W_ig, b_ii) +  F.linear(prev_h, W_hg, b_hg))\n",
        "o_t = torch.sigmoid(F.linear(input_x, W_io, b_ii) +  F.linear(prev_h, W_ho, b_ho))\n",
        "c_t = f_t * prev_c + i_t * g_t\n",
        "h_t = o_t * torch.tanh(c_t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0.1161]], grad_fn=<SigmoidBackward0>)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "i_t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "qdB62UN3DdFI"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[0.8456]], grad_fn=<SigmoidBackward0>),\n",
              " tensor([[-0.1159]], grad_fn=<AddBackward0>),\n",
              " tensor([[-0.0976]], grad_fn=<MulBackward0>))"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "o_t, c_t, h_t"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRkOgN_FEyl4"
      },
      "source": [
        "### 2.3 Comapre manual calculations with Pytorch implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PdDoF3sMVJ3",
        "outputId": "aa429ac2-b187-41b1-ce27-dd6418ab960e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([-0.0976], grad_fn=<SelectBackward0>),\n",
              " tensor([[-0.0976]], grad_fn=<MulBackward0>))"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output.squeeze(0)[0], h_t"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLhmYKNVqboO"
      },
      "source": [
        "**Task:** Calculate the outputs for the rest of the full sentence -> `simple_sequence` manually and compare with PyTorch output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMotALke62a1",
        "outputId": "3cb9f4e9-0c71-4c90-9dea-deaf3b3f20ff"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0.4000, 0.1000, 0.2300])"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "simple_sequence.squeeze(0).squeeze(0)[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kuivnScoOiLm",
        "outputId": "382a25e7-dde4-414f-f3e8-cc8a5a05b439"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[-0.0976]], grad_fn=<MulBackward0>)\n",
            "tensor([[0.0470]], grad_fn=<MulBackward0>)\n",
            "tensor([[0.0490]], grad_fn=<MulBackward0>)\n",
            "tensor([[0.1372]], grad_fn=<MulBackward0>)\n"
          ]
        }
      ],
      "source": [
        "prev_h = torch.zeros((1, hidden_size))\n",
        "prev_c = torch.zeros((1, hidden_size))\n",
        "\n",
        "for i in range(simple_sequence.shape[1]):\n",
        "  input_x = simple_sequence[0][i].unsqueeze(0)\n",
        "  i_t = torch.sigmoid(F.linear(input_x, W_ii, b_ii) +  F.linear(W_hi, prev_h, b_hi))\n",
        "  f_t = torch.sigmoid(F.linear(input_x, W_if, b_if) +  F.linear(prev_h, W_hf, b_hf))\n",
        "  g_t = torch.tanh(F.linear(input_x, W_ig, b_ii) +  F.linear(prev_h, W_hg, b_hg))\n",
        "  o_t = torch.sigmoid(F.linear(input_x, W_io, b_ii) +  F.linear(prev_h, W_ho, b_ho))\n",
        "  c_t = f_t * prev_c + i_t * g_t\n",
        "  h_t = o_t * torch.tanh(c_t)\n",
        "  prev_c = c_t\n",
        "  prev_h = h_t\n",
        "  print(h_t)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3HrBshLEMEk",
        "outputId": "7c156e89-1495-4569-9684-3d39c22e0bcd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[-0.0976],\n",
              "         [ 0.0470],\n",
              "         [ 0.0490],\n",
              "         [ 0.1372]]], grad_fn=<TransposeBackward0>)"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAwUl3tNOtgC"
      },
      "source": [
        "## 2. Application of LSTM (Sentiment Analysis)\n",
        "\n",
        "### 2.1 Dataset Description\n",
        "\n",
        "[IMDb dataset](http://ai.stanford.edu/~amaas/data/sentiment/) having 50K movie reviews for natural language processing or Text analytics. This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "LFMR6Zv8Ovw5"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "\n",
        "import datasets\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pePNLd1fPMf5"
      },
      "source": [
        "### 2.2 Get Dataset and preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "EZhcWJANPMG7"
      },
      "outputs": [],
      "source": [
        "train_data, test_data = datasets.load_dataset(\"imdb\", split=[\"train\", \"test\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utkyerxYPXaf"
      },
      "source": [
        "### 2.3 Tokenize Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "ok9kuURyPbKB"
      },
      "outputs": [],
      "source": [
        "max_text_length = 128\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Luvc9zxcPjb6"
      },
      "source": [
        "### 2.4 Create Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "cIBc3LQ7PgT1"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "def tokenize(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text) \n",
        "    return text.split()  \n",
        "\n",
        "\n",
        "def build_vocab(dataset, min_freq=5):\n",
        "    counter = Counter()\n",
        "    for example in dataset:\n",
        "        tokens = tokenize(example['text'])\n",
        "        counter.update(tokens)\n",
        "\n",
        "    # special_tokens = [\"<unk>\", \"<pad>\"]\n",
        "    vocab = {'<unk>': 0, '<pad>': 1} \n",
        "    idx = 2\n",
        "    for word, count in counter.items():\n",
        "        if count >= min_freq:\n",
        "            vocab[word] = idx\n",
        "            idx += 1\n",
        "\n",
        "    return vocab\n",
        "\n",
        "\n",
        "vocab = build_vocab(train_data, min_freq = 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myeU2qWbPmjv"
      },
      "source": [
        "### 2.5 Encode Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def encode_text(text, vocab):\n",
        "    tokens = tokenize(text)\n",
        "    return [vocab.get(token, vocab['<unk>']) for token in tokens]\n",
        "\n",
        "def encode_data(dataset, vocab):\n",
        "    encoded_data = []\n",
        "   \n",
        "    for example in dataset:\n",
        "        input_ids = encode_text(example['text'], vocab)\n",
        "        label = example['label']\n",
        "        encoded_data.append({'input_ids': input_ids, 'label': label})\n",
        "    return encoded_data\n",
        "\n",
        "train_encoded = encode_data(train_data, vocab)\n",
        "test_encoded = encode_data(test_data, vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1O7io-CjPs9m"
      },
      "source": [
        "### 2.6 Creating Dataloaders\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "mi3ulScaPwOc"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class ImdbDataset(Dataset):\n",
        "    def __init__(self, data, vocab, max_len=256):\n",
        "        self.data = data\n",
        "        self.vocab = vocab\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        example = self.data[idx]\n",
        "        input_ids = example['input_ids']\n",
        "        label = example['label']\n",
        "\n",
        "        # Padding or truncating to max_len\n",
        "        input_ids = input_ids[:self.max_len]  # Truncate to max_len\n",
        "        padding_len = self.max_len - len(input_ids)\n",
        "        input_ids = input_ids + [self.vocab['<pad>']] * padding_len  # Pad with <pad> token\n",
        "\n",
        "        return torch.tensor(input_ids), torch.tensor(label)\n",
        "\n",
        "# Create PyTorch datasets for train and test\n",
        "train_dataset = ImdbDataset(train_encoded, vocab, max_len = 128)\n",
        "test_dataset = ImdbDataset(test_encoded, vocab, max_len = 128)\n",
        "\n",
        "# Create dataloaders\n",
        "BATCH_SIZE = 64\n",
        "train_data_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_data_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skldsoALP3xB"
      },
      "source": [
        "### 2.7 Define LSTM model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "midQ02LJP4KW"
      },
      "outputs": [],
      "source": [
        "class SentimentLSTM(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, pad_index, n_layers=1, bidirectional=False):\n",
        "    super().__init__()\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_index)\n",
        "    self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, batch_first=True)\n",
        "    self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "  def forward(self, ids):\n",
        "      embedded = self.embedding(ids)\n",
        "      output, (hidden, cell) = self.lstm(embedded)\n",
        "      prediction = self.fc(hidden[-1])\n",
        "      return prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4OwfcvEQK1e"
      },
      "source": [
        "### 2.8 Model training parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "M6yhwVJ5QLNu"
      },
      "outputs": [],
      "source": [
        "vocab_size = len(vocab)\n",
        "embedding_dim = 300\n",
        "hidden_dim = 32\n",
        "output_dim = 2\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "lr = 5e-4\n",
        "pad_index = 2\n",
        "\n",
        "model = SentimentLSTM(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    hidden_dim=hidden_dim,\n",
        "    output_dim=output_dim,\n",
        "    pad_index=pad_index,\n",
        ").to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdmztP7MQYIN"
      },
      "source": [
        "### 2.9 Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2CAOfZ5bQgwm"
      },
      "outputs": [],
      "source": [
        "def get_accuracy(preds, labels):\n",
        "    if torch.is_tensor(preds):\n",
        "        preds = preds.cpu().numpy()\n",
        "    if torch.is_tensor(labels):\n",
        "        labels = labels.cpu().numpy()\n",
        "    correct = (preds == labels).sum()\n",
        "    return correct / len(labels)\n",
        "\n",
        "\n",
        "def evaluate(dataloader, model, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for input_ids, label in dataloader:\n",
        "            input_ids, label = input_ids.to(device), label.to(device)\n",
        "            output = model(input_ids)\n",
        "            loss = criterion(output, label)\n",
        "            total_loss += loss.item()\n",
        "            preds = torch.argmax(output, dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(label.cpu().numpy())\n",
        "    \n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    accuracy = get_accuracy(np.array(all_preds), np.array(all_labels))\n",
        "    print(f'Accuracy: {accuracy:.4f}')\n",
        "    return avg_loss, accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSNdTWWgQjjD"
      },
      "source": [
        "### 2.10 Model training Loop\n",
        "\n",
        "**Task** : Add model evaluation (use `test_data_loader`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "training...: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 391/391 [00:05<00:00, 72.50it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.5356\n",
            "[Epoch 1] Train Loss: 0.694, Train Acc: 0.515, Test Loss: 0.690, Test Acc: 0.536\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "training...: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 391/391 [00:05<00:00, 74.29it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.5692\n",
            "[Epoch 2] Train Loss: 0.671, Train Acc: 0.596, Test Loss: 0.679, Test Acc: 0.569\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "training...: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 391/391 [00:05<00:00, 74.96it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.7010\n",
            "[Epoch 3] Train Loss: 0.587, Train Acc: 0.705, Test Loss: 0.590, Test Acc: 0.701\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "training...: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 391/391 [00:05<00:00, 74.98it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.7479\n",
            "[Epoch 4] Train Loss: 0.452, Train Acc: 0.798, Test Loss: 0.548, Test Acc: 0.748\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "training...: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 391/391 [00:05<00:00, 73.70it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.7596\n",
            "[Epoch 5] Train Loss: 0.348, Train Acc: 0.858, Test Loss: 0.543, Test Acc: 0.760\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "training...: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 391/391 [00:05<00:00, 75.90it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.7578\n",
            "[Epoch 6] Train Loss: 0.271, Train Acc: 0.897, Test Loss: 0.576, Test Acc: 0.758\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "training...: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 391/391 [00:05<00:00, 74.67it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.7710\n",
            "[Epoch 7] Train Loss: 0.214, Train Acc: 0.923, Test Loss: 0.617, Test Acc: 0.771\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "training...: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 391/391 [00:05<00:00, 67.26it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.7684\n",
            "[Epoch 8] Train Loss: 0.156, Train Acc: 0.947, Test Loss: 0.642, Test Acc: 0.768\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "training...: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 391/391 [00:06<00:00, 60.09it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.7572\n",
            "[Epoch 9] Train Loss: 0.143, Train Acc: 0.953, Test Loss: 0.673, Test Acc: 0.757\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "training...: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 391/391 [00:06<00:00, 60.95it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.7612\n",
            "[Epoch 10] Train Loss: 0.117, Train Acc: 0.962, Test Loss: 0.694, Test Acc: 0.761\n"
          ]
        }
      ],
      "source": [
        "n_epochs = 10\n",
        "for ep in range(n_epochs):\n",
        "    model.train()\n",
        "    epoch_loss = []\n",
        "    epoch_acc = []\n",
        "    \n",
        "    for batch in tqdm.tqdm(train_data_loader, desc=\"training...\"):\n",
        "        optimizer.zero_grad()\n",
        "        ids, label = batch[0], batch[1]\n",
        "        ids, label = ids.to(device), label.to(device)\n",
        "        \n",
        "        prediction = model(ids)\n",
        "        loss = criterion(prediction, label)\n",
        "        \n",
        "        preds = torch.argmax(prediction, dim=1)\n",
        "        acc = get_accuracy(preds, label)\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss.append(loss.item())\n",
        "        epoch_acc.append(acc)\n",
        "    \n",
        "    avg_train_loss = np.mean(epoch_loss)\n",
        "    avg_train_acc = np.mean(epoch_acc)\n",
        "    \n",
        "    test_loss, test_acc = evaluate(test_data_loader, model, criterion=criterion, device=device)\n",
        "    print(f'[Epoch {ep+1}] Train Loss: {avg_train_loss:.3f}, Train Acc: {avg_train_acc:.3f}, Test Loss: {test_loss:.3f}, Test Acc: {test_acc:.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32KOGoWTRSMV"
      },
      "source": [
        "## 3. Tasks\n",
        "\n",
        "```\n",
        "Task 1\n",
        "Implement and train a LSTM neural network for sentiment analysis using IMDb dataset and the following architecture:\n",
        "- LSTM should be bidirectional\n",
        "- LSTM should be Multi-layered\n",
        "- LSTM should be use Regularization (i.e Dropout)\n",
        "```\n",
        "\n",
        "<hr>\n",
        "\n",
        "```\n",
        "Task 2\n",
        "Implement, train and test a LSTM model for Part-of-speech tagging task.\n",
        "```\n",
        "\n",
        "**Task 2 Datasets**: [Train](https://www.dropbox.com/s/x9n6f9o9jl7pno8/train_pos.txt?dl=1), [Test](https://www.dropbox.com/s/v8nccvq7jewcl8s/test_pos.txt?dl=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import datasets\n",
        "from collections import Counter\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [],
      "source": [
        "class IMDBDataset(Dataset):\n",
        "    def __init__(self, split='train', min_freq=2, max_len=256):\n",
        "        self.max_len = max_len\n",
        "        self.min_freq = min_freq\n",
        "\n",
        "        data = list(datasets.load_dataset(\"imdb\", split=split))\n",
        "        self.texts = []\n",
        "        self.labels = []  # 1 for positive, 0 for negative\n",
        "        for label, text in data:\n",
        "            self.texts.append(text)\n",
        "            self.labels.append(int(label == 'pos'))\n",
        "        \n",
        "        self.vocab = self.build_vocab(self.texts, min_freq)\n",
        "        self.encoded_texts = [self.encode_text(text) for text in self.texts]\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        text = text.lower()\n",
        "        text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
        "        return text.split()  # Tokenize by whitespace\n",
        "    \n",
        "    def build_vocab(self, texts, min_freq):\n",
        "        counter = Counter()\n",
        "        for text in texts:\n",
        "            tokens = self.tokenize(text)\n",
        "            counter.update(tokens)\n",
        "        vocab = {'<pad>': 0, '<unk>': 1}\n",
        "        idx = 2\n",
        "        for token, count in counter.items():\n",
        "            if count >= min_freq:\n",
        "                vocab[token] = idx\n",
        "                idx += 1\n",
        "        return vocab\n",
        "    \n",
        "    def encode_text(self, text):\n",
        "        tokens = self.tokenize(text)\n",
        "        return [self.vocab.get(token, self.vocab['<unk>']) for token in tokens]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.encoded_texts)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        encoded = self.encoded_texts[idx]\n",
        "        if len(encoded) < self.max_len:\n",
        "            pad_len = self.max_len - len(encoded)\n",
        "            encoded = encoded + [self.vocab['<pad>']] * pad_len\n",
        "        else:\n",
        "            encoded = encoded[:self.max_len]\n",
        "        label = self.labels[idx]\n",
        "        return torch.tensor(encoded, dtype=torch.long), torch.tensor(label, dtype=torch.long)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SentimentLSTMModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, output_dim, dropout=0.5):\n",
        "        super(SentimentLSTMModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(input_size=embedding_dim,\n",
        "                            hidden_size=hidden_dim,\n",
        "                            num_layers=num_layers,\n",
        "                            bidirectional=True,\n",
        "                            dropout=dropout,\n",
        "                            batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, text):\n",
        "        # text: [batch_size, seq_len]\n",
        "        embedded = self.embedding(text)           # [batch_size, seq_len, embedding_dim]\n",
        "        embedded = self.dropout(embedded)\n",
        "        output, (hidden, cell) = self.lstm(embedded)\n",
        "        forward_hidden = hidden[-2, :, :]          # [batch_size, hidden_dim]\n",
        "        backward_hidden = hidden[-1, :, :]         # [batch_size, hidden_dim]\n",
        "        final_hidden = torch.cat((forward_hidden, backward_hidden), dim=1)  # [batch_size, hidden_dim*2]\n",
        "        final_hidden = self.dropout(final_hidden)\n",
        "        logits = self.fc(final_hidden)            # [batch_size, output_dim]\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SentimentTrainer:\n",
        "    def __init__(self, model, learning_rate):\n",
        "        self.model = model\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
        "        print(f\"Using device: {self.device}\")\n",
        "\n",
        "    def train(self, dataset, num_epochs=5, batch_size=64):\n",
        "        self.model.to(self.device)\n",
        "        self.model.train()\n",
        "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "        for epoch in range(num_epochs):\n",
        "            total_loss = 0.0\n",
        "            for text, label in dataloader:\n",
        "                text, label = text.to(self.device), label.to(self.device)\n",
        "                self.optimizer.zero_grad()\n",
        "                logits = self.model(text)  # [batch_size, output_dim]\n",
        "                loss = self.criterion(logits, label)\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "                total_loss += loss.item()\n",
        "            print(f'Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(dataloader):.4f}')\n",
        "\n",
        "    def evaluate(self, dataset, batch_size=64):\n",
        "        self.model.eval()\n",
        "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "        total_correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for text, label in dataloader:\n",
        "                text, label = text.to(self.device), label.to(self.device)\n",
        "                logits = self.model(text)\n",
        "                predictions = torch.argmax(logits, dim=1)\n",
        "                total_correct += (predictions == label).sum().item()\n",
        "                total += label.size(0)\n",
        "        accuracy = total_correct / total\n",
        "        print(f'IMDb Sentiment Accuracy: {accuracy:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SentimentLSTMModel(\n",
            "  (embedding): Embedding(3, 150)\n",
            "  (lstm): LSTM(150, 128, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)\n",
            "  (fc): Linear(in_features=256, out_features=2, bias=True)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            ")\n",
            "Using device: cuda\n",
            "Epoch 1/5, Loss: 0.0751\n",
            "Epoch 2/5, Loss: 0.0004\n",
            "Epoch 3/5, Loss: 0.0002\n",
            "Epoch 4/5, Loss: 0.0001\n",
            "Epoch 5/5, Loss: 0.0001\n",
            "IMDb Sentiment Accuracy: 1.0000\n"
          ]
        }
      ],
      "source": [
        "imdb_train = IMDBDataset(split='train', min_freq=2, max_len=256)\n",
        "imdb_test = IMDBDataset(split='test', min_freq=2, max_len=256)\n",
        "\n",
        "vocab_size_imdb = len(imdb_train.vocab)\n",
        "output_dim = 2  # Binary classification (neg, pos)\n",
        "embedding_dim = 150\n",
        "hidden_dim = 128\n",
        "num_layers = 2\n",
        "dropout = 0.5\n",
        "learning_rate = 0.0001\n",
        "\n",
        "sentiment_model = SentimentLSTMModel(vocab_size_imdb, embedding_dim, hidden_dim, num_layers, output_dim, dropout)\n",
        "print(sentiment_model)\n",
        "\n",
        "sentiment_trainer = SentimentTrainer(sentiment_model, learning_rate)\n",
        "sentiment_trainer.train(imdb_train, num_epochs=5, batch_size=128)\n",
        "sentiment_trainer.evaluate(imdb_test, batch_size=128)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class POSTagDataset(Dataset):\n",
        "    def __init__(self, filepath, min_freq=1, max_len=50, vocab=None, tag_vocab=None):\n",
        "        self.min_freq = min_freq\n",
        "        self.max_len = max_len\n",
        "        self.sentences, self.tags = self.read_file(filepath)\n",
        "        if vocab is None:\n",
        "            self.vocab = self.build_vocab(self.sentences, min_freq)\n",
        "        else:\n",
        "            self.vocab = vocab\n",
        "        if tag_vocab is None:\n",
        "            self.tag_vocab = self.build_tag_vocab(self.tags)\n",
        "        else:\n",
        "            self.tag_vocab = tag_vocab\n",
        "        self.encoded_data = self.encode_data()\n",
        "\n",
        "    def read_file(self, filepath):\n",
        "        sentences = []\n",
        "        tags = []\n",
        "        current_sentence = []\n",
        "        current_tags = []\n",
        "        with open(filepath, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                if line == \"\":\n",
        "                    if current_sentence:\n",
        "                        sentences.append(current_sentence)\n",
        "                        tags.append(current_tags)\n",
        "                        current_sentence = []\n",
        "                        current_tags = []\n",
        "                else:\n",
        "                    parts = line.split()\n",
        "                    if len(parts) == 2:\n",
        "                        word, tag = parts\n",
        "                        current_sentence.append(word)\n",
        "                        current_tags.append(tag)\n",
        "        if current_sentence:\n",
        "            sentences.append(current_sentence)\n",
        "            tags.append(current_tags)\n",
        "        return sentences, tags\n",
        "\n",
        "    def build_vocab(self, sentences, min_freq):\n",
        "        counter = Counter()\n",
        "        for sent in sentences:\n",
        "            counter.update(sent)\n",
        "        vocab = {'<pad>': 0, '<unk>': 1}\n",
        "        idx = 2\n",
        "        for word, count in counter.items():\n",
        "            if count >= min_freq:\n",
        "                vocab[word] = idx\n",
        "                idx += 1\n",
        "        return vocab\n",
        "\n",
        "    def build_tag_vocab(self, tag_lists):\n",
        "        counter = Counter()\n",
        "        for tag_list in tag_lists:\n",
        "            counter.update(tag_list)\n",
        "        tag_vocab = {'<pad>': 0}\n",
        "        idx = 1\n",
        "        for tag in sorted(counter.keys()):\n",
        "            tag_vocab[tag] = idx\n",
        "            idx += 1\n",
        "        return tag_vocab\n",
        "\n",
        "    def encode_sentence(self, sentence, vocab):\n",
        "        return [vocab.get(word, vocab['<unk>']) for word in sentence]\n",
        "\n",
        "    def encode_tags(self, tag_list, tag_vocab):\n",
        "        return [tag_vocab[tag] for tag in tag_list]\n",
        "\n",
        "    def encode_data(self):\n",
        "        encoded_data = []\n",
        "        for sent, tag_list in zip(self.sentences, self.tags):\n",
        "            encoded_sent = self.encode_sentence(sent, self.vocab)\n",
        "            encoded_tag = self.encode_tags(tag_list, self.tag_vocab)\n",
        "            if len(encoded_sent) < self.max_len:\n",
        "                pad_len = self.max_len - len(encoded_sent)\n",
        "                encoded_sent = encoded_sent + [self.vocab['<pad>']] * pad_len\n",
        "                encoded_tag = encoded_tag + [self.tag_vocab['<pad>']] * pad_len\n",
        "            else:\n",
        "                encoded_sent = encoded_sent[:self.max_len]\n",
        "                encoded_tag = encoded_tag[:self.max_len]\n",
        "            encoded_data.append({'input_ids': encoded_sent, 'tags': encoded_tag})\n",
        "        return encoded_data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encoded_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        example = self.encoded_data[idx]\n",
        "        input_ids = torch.tensor(example['input_ids'], dtype=torch.long)\n",
        "        tags = torch.tensor(example['tags'], dtype=torch.long)\n",
        "        return input_ids, tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "class POSTagLSTMModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, num_tags, dropout=0.5):\n",
        "        super(POSTagLSTMModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(input_size=embedding_dim,\n",
        "                            hidden_size=hidden_dim,\n",
        "                            num_layers=num_layers,\n",
        "                            bidirectional=True,\n",
        "                            dropout=dropout,\n",
        "                            batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, num_tags)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        # input_ids: [batch_size, seq_len]\n",
        "        embedded = self.embedding(input_ids)   # [batch_size, seq_len, embedding_dim]\n",
        "        outputs, _ = self.lstm(embedded)          # [batch_size, seq_len, hidden_dim*2]\n",
        "        logits = self.fc(outputs)                # [batch_size, seq_len, num_tags]\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "UXaYxAD7HkYC"
      },
      "outputs": [],
      "source": [
        "class POSTrainer:\n",
        "    def __init__(self, model, learning_rate):\n",
        "        self.model = model\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
        "        print(f\"Using device: {self.device}\")\n",
        "\n",
        "    def train(self, dataset, num_epochs=5, batch_size=32):\n",
        "        self.model.to(self.device)\n",
        "        self.model.train()\n",
        "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "        for epoch in range(num_epochs):\n",
        "            total_loss = 0.0\n",
        "            for input_ids, tags in dataloader:\n",
        "                input_ids, tags = input_ids.to(self.device), tags.to(self.device)\n",
        "                self.optimizer.zero_grad()\n",
        "                logits = self.model(input_ids)  # [batch_size, seq_len, num_tags]\n",
        "                loss = self.criterion(logits.view(-1, logits.shape[-1]), tags.view(-1))\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "                total_loss += loss.item()\n",
        "            print(f'Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(dataloader):.4f}')\n",
        "\n",
        "    def evaluate(self, dataset, batch_size=32):\n",
        "        self.model.eval()\n",
        "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "        all_preds = []\n",
        "        all_tags = []\n",
        "        with torch.no_grad():\n",
        "            for input_ids, tags in dataloader:\n",
        "                input_ids, tags = input_ids.to(self.device), tags.to(self.device)\n",
        "                logits = self.model(input_ids)  # [batch_size, seq_len, num_tags]\n",
        "                preds = logits.argmax(dim=-1)\n",
        "                mask = tags != 0\n",
        "                all_preds.extend(preds[mask].cpu().numpy())\n",
        "                all_tags.extend(tags[mask].cpu().numpy())\n",
        "        accuracy = accuracy_score(all_tags, all_preds)\n",
        "        print(f'POS Tagging Accuracy: {accuracy:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "POSTagLSTMModel(\n",
              "  (embedding): Embedding(19124, 120)\n",
              "  (lstm): LSTM(120, 128, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)\n",
              "  (fc): Linear(in_features=256, out_features=45, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_pos_filepath = \"Labs/lab5/train_pos.txt\"\n",
        "test_pos_filepath = \"Labs/lab5/test_pos.txt\"\n",
        "\n",
        "\n",
        "pos_train_dataset = POSTagDataset(train_pos_filepath, min_freq=1, max_len=50)\n",
        "pos_test_dataset = POSTagDataset(test_pos_filepath, min_freq=1, max_len=50,\n",
        "                                    vocab=pos_train_dataset.vocab,\n",
        "                                    tag_vocab=pos_train_dataset.tag_vocab)\n",
        "\n",
        "vocab_size_pos = len(pos_train_dataset.vocab)\n",
        "num_tags = len(pos_train_dataset.tag_vocab)\n",
        "pos_embedding_dim = 120\n",
        "pos_hidden_dim = 128\n",
        "pos_num_layers = 2\n",
        "pos_dropout = 0.5\n",
        "pos_learning_rate = 0.001\n",
        "\n",
        "pos_model = POSTagLSTMModel(vocab_size_pos, pos_embedding_dim, pos_hidden_dim, pos_num_layers, num_tags, pos_dropout)\n",
        "pos_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Epoch 1/5, Loss: 1.3322\n",
            "Epoch 2/5, Loss: 0.4983\n",
            "Epoch 3/5, Loss: 0.3163\n",
            "Epoch 4/5, Loss: 0.2223\n",
            "Epoch 5/5, Loss: 0.1635\n",
            "POS Tagging Accuracy: 0.9261\n"
          ]
        }
      ],
      "source": [
        "pos_trainer = POSTrainer(pos_model, pos_learning_rate)\n",
        "pos_trainer.train(pos_train_dataset, num_epochs=5, batch_size=32)\n",
        "pos_trainer.evaluate(pos_test_dataset, batch_size=32)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

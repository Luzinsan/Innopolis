{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JIu6A55Y0x_"
      },
      "source": [
        "# Lab 6: Transformers and LLMs\n",
        "```\n",
        "- [S25] Advanced Machine Learning, Innopolis University\n",
        "- Teaching Assistant: Gcinizwe Dlamini\n",
        "```\n",
        "<hr>\n",
        "\n",
        "\n",
        "```\n",
        "Lab Plan\n",
        "1. Transformers (translation architecture)\n",
        "2. Self-Attention\n",
        "3. Multi-headed attention\n",
        "4. Positional Encoding\n",
        "5. Transfomer Encoder Part\n",
        "6. Application of Transformers\n",
        "7. Self practice tasks\n",
        "```\n",
        "\n",
        "<hr>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4VPwrSfZabH"
      },
      "source": [
        "# 1. Transformers\n",
        "\n",
        "* [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf) -- Original paper on attention\n",
        "\n",
        "![](http://jalammar.github.io/images/t/The_transformer_encoder_decoder_stack.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUQqqmffZmgL"
      },
      "source": [
        "## 1.1 Transfomer Encoder\n",
        "\n",
        "The encoder contains a self-attention layer – a layer that helps the encoder look at other words in the input sentence as it encodes a specific word. <br>\n",
        "PyTorch implementation : `nn.TransformerEncoder` and `nn.TransformerEncoderLayer` <br>\n",
        "**The main goal is to efficiently encode the data**\n",
        "\n",
        "  1         |  2\n",
        ":-------------------------:|:-------------------------:\n",
        "![](http://jalammar.github.io/images/t/encoder_with_tensors.png)  |  ![](http://jalammar.github.io/images/t/transformer_resideual_layer_norm_2.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krH5CoxGdHnD"
      },
      "source": [
        "## 2. Self-Attention\n",
        "\n",
        "**Keep in mind : The main goal is to encode the data in a much more efficient way** In other words is to create meaningful embeddings<br>\n",
        "- As the model processes each word (each position in the input sequence), self attention allows it to look at other positions in the input sequence for clues that can help lead to a better encoding for this word.\n",
        "\n",
        "\n",
        "**How does Self-Attention work?**\n",
        "\n",
        "Steps:\n",
        "1. For each word, we create a **`Query`** vector, a **`Key`** vector, and a **`Value`** vector.\n",
        "  - What are the **`Query`** vector, a **`Key`** vector, and a **`Value`** vector? : They're abstractions that are useful for calculating attention... They are a breakdown of the word embeddings\n",
        "2. Calculating self-attention score from **`Query`** **`Key`** vector.\n",
        "3. Divide the scores by 8 (This leads to having more stable gradients)\n",
        "4. Pass the result through a softmax operation (softmax score determines how much each word will be expressed at this position)\n",
        "5. Multiply each value vector by the softmax score\n",
        "6. Sum up the weighted value vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-H7Wxnp2eTeJ"
      },
      "source": [
        "### Step 1\n",
        "\n",
        "For each word, we create a **`Query`** vector, a **`Key`** vector, and a **`Value`** vector.\n",
        "\n",
        "![](http://jalammar.github.io/images/t/transformer_self_attention_vectors.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BI7XGeySYtV2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# simple sequence = I will pass AML midterm\n",
        "simple_sequence_embedding = torch.rand((5, 512))\n",
        "\n",
        "# Create weight matrices\n",
        "Wq = torch.normal(0,0.5, (512, 7))\n",
        "Wk = torch.normal(0,0.1, (512, 7))\n",
        "Wv = torch.normal(0,0.2, (512, 7))\n",
        "\n",
        "\n",
        "# Create key, query and value for each word in the senetence\n",
        "queries = simple_sequence_embedding @ Wq\n",
        "keys = simple_sequence_embedding @ Wk\n",
        "values = simple_sequence_embedding @ Wv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6E3yaeQEed7-"
      },
      "source": [
        "### Step 2\n",
        "\n",
        "Calculating self-attention score from **`Query`** and **`Key`** vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8yABUd2Qd818"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[30.1358, 18.4392, 29.8348, 40.2687, 42.2601],\n",
              "        [17.9468,  2.4468, 16.2118, 18.4139, 19.1587],\n",
              "        [26.3514, 15.4484, 33.9176, 44.3448, 40.3456],\n",
              "        [11.0957, -6.5896,  3.6480, 16.1486,  6.7803],\n",
              "        [29.3687, 25.3186, 33.1763, 60.3646, 55.5537]])"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "scores = queries @ keys.T\n",
        "scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPM5PoSHer3k"
      },
      "source": [
        "### Step 3\n",
        "Divide the scores by 8 (This leads to having more stable gradients)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Iafga235ekiK"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[11.3903,  6.9694, 11.2765, 15.2201, 15.9728],\n",
              "        [ 6.7833,  0.9248,  6.1275,  6.9598,  7.2413],\n",
              "        [ 9.9599,  5.8390, 12.8196, 16.7608, 15.2492],\n",
              "        [ 4.1938, -2.4906,  1.3788,  6.1036,  2.5627],\n",
              "        [11.1003,  9.5695, 12.5394, 22.8157, 20.9973]])"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "scores = scores / (keys.shape[-1] ** 0.5)\n",
        "scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsuoztzrexQU"
      },
      "source": [
        "### Step 4\n",
        "\n",
        "Pass the result through a softmax operation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "0n5NWeN_ez_D"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[6.8623e-03, 8.2508e-05, 6.1245e-03, 3.1606e-01, 6.7087e-01],\n",
              "        [2.3278e-01, 6.6473e-04, 1.2082e-01, 2.7772e-01, 3.6801e-01],\n",
              "        [8.9660e-04, 1.4551e-05, 1.5652e-02, 8.0573e-01, 1.7771e-01],\n",
              "        [1.2487e-01, 1.5611e-04, 7.4802e-03, 8.4306e-01, 2.4438e-02],\n",
              "        [7.0269e-06, 1.5204e-06, 2.9632e-05, 8.6033e-01, 1.3963e-01]])"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch.nn.functional as f\n",
        "scores = f.softmax(scores, dim=-1)\n",
        "scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uzXnkD-e4t4"
      },
      "source": [
        "### Step 5 & 6\n",
        "\n",
        "* Multiply each value vector by the softmax score\n",
        "* Sum up the weighted value vectors\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "YpcDWsBhe6Ma"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[-1.7536e+00, -2.5648e+00,  1.2177e+00, -4.7075e+00, -1.1307e+00,\n",
              "          3.4390e+00, -1.0637e+00],\n",
              "        [-1.5368e+00, -2.6449e+00,  1.3245e+00, -4.7680e+00, -1.5274e+00,\n",
              "          3.0759e+00, -1.0441e+00],\n",
              "        [-8.2911e-01, -1.9917e+00, -1.8432e-01, -5.0140e+00, -1.1249e-01,\n",
              "          3.5235e+00, -6.4149e-01],\n",
              "        [-6.5421e-01, -1.9699e+00, -1.6939e-01, -5.2413e+00, -2.3667e-01,\n",
              "          3.4747e+00, -5.0592e-01],\n",
              "        [-7.3888e-01, -1.9283e+00, -3.1250e-01, -5.0841e+00, -4.7249e-03,\n",
              "          3.5655e+00, -5.7742e-01]])"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "z = scores @ values\n",
        "z"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_his4tXgPk1"
      },
      "source": [
        "## 3. Multi-headed attention\n",
        "\n",
        "**GOAL**:\n",
        "1. Expand the model's ability to focus on different positions\n",
        "2. Provide the attention layer multiple “representation subspaces”\n",
        "\n",
        "**Attention with $N$ just means repeating self attention algorithm $N$ times and joining the results**\n",
        "\n",
        "\n",
        "![](https://data-science-blog.com/wp-content/uploads/2022/01/mha_img_original.png)\n",
        "\n",
        "**Multi-headed attention steps:**\n",
        "1. Same as self-attention calculation, just n different times with different weight matrices\n",
        "2. Condense the $N$ z metrices down into a single matrix by concatinating the matrices then multiply them by an additional weights matrix `WO`\n",
        "\n",
        "Now the output z metrix is fed to the FFNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "YTwNl_mJfAot"
      },
      "outputs": [],
      "source": [
        "from torch import Tensor\n",
        "import torch.nn.functional as f\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "def scaled_dot_product_attention(query, key, value):\n",
        "    temp = query.bmm(key.transpose(1, 2))\n",
        "    scale = query.size(-1) ** 0.5\n",
        "    softmax = f.softmax(temp / scale, dim=-1)\n",
        "    return softmax.bmm(value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMHINHLkglES"
      },
      "source": [
        "### 3.1 Attention head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "H2QBY47ogoEE"
      },
      "outputs": [],
      "source": [
        "class AttentionHead(nn.Module):\n",
        "    def __init__(self, dim_in, dim_q, dim_k):\n",
        "        super().__init__()\n",
        "        self.q = nn.Linear(dim_in, dim_q)\n",
        "        self.k = nn.Linear(dim_in, dim_k)\n",
        "        self.v = nn.Linear(dim_in, dim_k)\n",
        "\n",
        "    def forward(self, query, key, value):\n",
        "        return scaled_dot_product_attention(self.q(query), self.k(key), self.v(value))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyi73Namg0sM"
      },
      "source": [
        "### 3.2 Multi Head Attention\n",
        "\n",
        "**Task:** Implement multi-head attention model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "uw01PJMZg3yc"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, number_of_heads, dim_in, dim_q, dim_k):\n",
        "      super().__init__()\n",
        "      self.heads = nn.ModuleList([AttentionHead(dim_in, dim_q, dim_k) for _ in range(number_of_heads)])\n",
        "      self.linear = nn.Linear(number_of_heads * dim_k, dim_in)\n",
        "\n",
        "  def forward(self, query, key, value):\n",
        "      z = self.linear(torch.cat([h(query, key, value) for h in self.heads], dim=-1))\n",
        "      return z\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MkY2jNhg2c7"
      },
      "source": [
        "## 4. Positional Encoding\n",
        "\n",
        "A way to account for the order of the words in the input sequence. A transformer adds a vector to each input embedding which helps it determine the position of each word. <br>\n",
        "**Goal** : preserving information about the order of tokens  \n",
        "\n",
        "Positional Encoding they can either be learned or fixed a priori.\n",
        "\n",
        "Proposed approach from original paper : describe a simple scheme for fixed positional encodings based on sine and cosine functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "cXIr-0-1i0zd"
      },
      "outputs": [],
      "source": [
        "def position_encoding(seq_len, dim_model, device):\n",
        "    pos = torch.arange(seq_len, dtype=torch.float, device=device).reshape(1, -1, 1)\n",
        "    dim = torch.arange(dim_model, dtype=torch.float, device=device).reshape(1, 1, -1)\n",
        "    phase = pos / (1e4 ** (dim / dim_model))\n",
        "\n",
        "    return torch.where(dim.long() % 2 == 0, torch.sin(phase), torch.cos(phase))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vi3aQYNznBWI"
      },
      "source": [
        "![](http://jalammar.github.io/images/t/transformer_resideual_layer_norm_2.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfCA86ACnH01"
      },
      "source": [
        "## 5. Transfomer Encoder Part\n",
        "### 5.1Encoder Feed Forward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ee2SB_sInB59"
      },
      "outputs": [],
      "source": [
        "def feed_forward(dim_input = 512, dim_feedforward = 2048):\n",
        "    return nn.Sequential(nn.Linear(dim_input, dim_feedforward), nn.ReLU(), nn.Linear(dim_feedforward, dim_input))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpQAHDPbnOiA"
      },
      "source": [
        "### 5.2 Encoder Residual\n",
        "\n",
        "From the original paper the author implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "j3z2wjR5nRn1"
      },
      "outputs": [],
      "source": [
        "class Residual(nn.Module):\n",
        "  def __init__(self, sublayer, dimension, dropout = 0.1):\n",
        "    super().__init__()\n",
        "    self.sublayer = sublayer\n",
        "    self.norm = nn.LayerNorm(dimension)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, *tensors):\n",
        "    return self.norm(tensors[0] + self.dropout(self.sublayer(*tensors)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxhYxI4Fnbnc"
      },
      "source": [
        "### 5.3 Putting the Encoder layer together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "lZz2JhflniHT"
      },
      "outputs": [],
      "source": [
        "class TransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self, dim_model = 512, num_heads = 6, dim_feedforward = 2048, dropout = 0.1):\n",
        "        super().__init__()\n",
        "        dim_q = dim_k = max(dim_model // num_heads, 1)\n",
        "        self.attention = Residual(MultiHeadAttention(num_heads, dim_model, dim_q, dim_k),\n",
        "                                  dimension=dim_model, dropout=dropout)\n",
        "        self.feed_forward = Residual(feed_forward(dim_model, dim_feedforward), dimension=dim_model, dropout=dropout)\n",
        "\n",
        "    def forward(self, src):\n",
        "        src = self.attention(src, src, src)\n",
        "        return self.feed_forward(src)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUGS-3opnlxg"
      },
      "source": [
        "## 5.4 Putting together transfomer Encoder part"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "QplT81gNnie2"
      },
      "outputs": [],
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, num_layers = 12, dim_model = 512, num_heads = 4, dim_feedforward = 2048, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([TransformerEncoderLayer(dim_model, num_heads, dim_feedforward, dropout) for _ in range(num_layers) ])\n",
        "\n",
        "    def forward(self, src):\n",
        "        seq_len, dimension = src.size(1), src.size(2)\n",
        "        src += position_encoding(seq_len, dimension, src.device)\n",
        "        for layer in self.layers:\n",
        "            src = layer(src)\n",
        "\n",
        "        return src"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuafKkNanzV3"
      },
      "source": [
        "# The Decoder Side\n",
        "\n",
        "The encoder start by processing the input sequence. The output of the top encoder is then transformed into a set of attention vectors K and V. These are to be used by each decoder.\n",
        "\n",
        "The “Encoder-Decoder Attention” layer works just like multiheaded self-attention, except it creates its Queries matrix from the layer below it, and takes the `Keys` and `Values` matrix from the output of the encoder stack.\n",
        "\n",
        "![](https://jalammar.github.io/images/t/transformer_resideual_layer_norm_3.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjwjs_zzn5Le"
      },
      "source": [
        "## Decoder layer\n",
        "\n",
        "\n",
        "\n",
        "**Task**: implement the decoder layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "ZfrwtIdOn1pT"
      },
      "outputs": [],
      "source": [
        "class TransformerDecoderLayer(nn.Module):\n",
        "    def __init__(self, dim_model=512, num_heads=6, dim_feedforward=2048, dropout=0.1):\n",
        "        super().__init__()\n",
        "        dim_q = dim_k = max(dim_model // num_heads, 1)\n",
        "        self.self_attention = Residual(MultiHeadAttention(num_heads, dim_model, dim_q, dim_k), dimension=dim_model, dropout=dropout)\n",
        "        self.cross_attention = Residual(MultiHeadAttention(num_heads, dim_model, dim_q, dim_k), dimension=dim_model, dropout=dropout)\n",
        "        self.feed_forward = Residual(feed_forward(dim_model, dim_feedforward), dimension=dim_model, dropout=dropout)\n",
        "\n",
        "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None):\n",
        "        tgt = self.self_attention(tgt, tgt, tgt, tgt_mask)\n",
        "        tgt = self.cross_attention(tgt, memory, memory, memory_mask)\n",
        "        return self.feed_forward(tgt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nafOjPVpsDR"
      },
      "source": [
        "## 6. Application of Transfomers\n",
        "\n",
        "We will look at sentiment analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "_mcCQsUQp4-u"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, tag_count: int):\n",
        "        super().__init__()\n",
        "        self.transformer = transformers.AutoModel.from_pretrained('bert-base-uncased')\n",
        "        for param in self.transformer.parameters():\n",
        "            param.requires_grad = False\n",
        "        hidden_dim = self.transformer.config.hidden_size\n",
        "        self.fc = nn.Linear(hidden_dim, tag_count)\n",
        "\n",
        "    def forward(self, input_ids: torch.Tensor) -> torch.Tensor:\n",
        "        attention_mask = (input_ids != 0).long()\n",
        "        output = self.transformer(input_ids, attention_mask=attention_mask, output_attentions=True)\n",
        "        hidden = output.last_hidden_state  # [batch_size, seq_len, hidden_dim]\n",
        "        logits = self.fc(torch.tanh(hidden))  # [batch_size, seq_len, tag_count]\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqLHQt0iog5v"
      },
      "source": [
        "## 7. Tasks\n",
        "\n",
        "```\n",
        "Task 1\n",
        "- Using the above implementation code the decoder layer and assemble a full transformer model\n",
        "```\n",
        "\n",
        "<hr>\n",
        "\n",
        "```\n",
        "Task 2\n",
        "- Implement, train and test a Transfomer model (use pytorch layers)for Part-of-speech tagging task.\n",
        "```\n",
        "\n",
        "**Task 2 Datasets**: [Train](https://www.dropbox.com/s/x9n6f9o9jl7pno8/train_pos.txt?dl=1), [Test](https://www.dropbox.com/s/v8nccvq7jewcl8s/test_pos.txt?dl=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from collections import Counter\n",
        "from sklearn.metrics import accuracy_score, classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [],
      "source": [
        "class POSTagDataset(Dataset):\n",
        "    def __init__(self, filepath: str, max_len: int = 50, tag_vocab: dict = None):\n",
        "        self.max_len = max_len\n",
        "        self.sentences, self.tags = self.read_file(filepath)\n",
        "        self.tokenizer = transformers.AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "        if tag_vocab is None:\n",
        "            self.tag_vocab = self.build_tag_vocab(self.tags)\n",
        "        else:\n",
        "            self.tag_vocab = tag_vocab\n",
        "        self.encoded_data = self.encode_data()\n",
        "\n",
        "    def read_file(self, filepath: str):\n",
        "        sentences = []\n",
        "        tags = []\n",
        "        current_sentence = []\n",
        "        current_tags = []\n",
        "        with open(filepath, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                if line == \"\":\n",
        "                    if current_sentence:\n",
        "                        sentences.append(current_sentence)\n",
        "                        tags.append(current_tags)\n",
        "                        current_sentence, current_tags = [], []\n",
        "                else:\n",
        "                    parts = line.split()\n",
        "                    if len(parts) == 2:\n",
        "                        word, tag = parts\n",
        "                        current_sentence.append(word)\n",
        "                        current_tags.append(tag)\n",
        "        if current_sentence:\n",
        "            sentences.append(current_sentence)\n",
        "            tags.append(current_tags)\n",
        "        return sentences, tags\n",
        "\n",
        "    def build_tag_vocab(self, tag_lists) -> dict:\n",
        "        counter = Counter()\n",
        "        for tag_list in tag_lists:\n",
        "            counter.update(tag_list)\n",
        "        tag_vocab = {'<PAD>': 0}\n",
        "        idx = 1\n",
        "        for tag in sorted(counter.keys()):\n",
        "            tag_vocab[tag] = idx\n",
        "            idx += 1\n",
        "        return tag_vocab\n",
        "\n",
        "    def encode_data(self) -> list:\n",
        "        encoded_data = []\n",
        "        for sent, tag_seq in zip(self.sentences, self.tags):\n",
        "            encoded = self.tokenizer(\n",
        "                sent,\n",
        "                is_split_into_words=True,\n",
        "                max_length=self.max_len,\n",
        "                padding='max_length',\n",
        "                truncation=True,\n",
        "                return_tensors='pt'\n",
        "            )\n",
        "            input_ids = encoded['input_ids'].squeeze(0)\n",
        "            word_ids = encoded.word_ids()\n",
        "            encoded_tags = []\n",
        "            for word_idx in word_ids:\n",
        "                if word_idx is None:\n",
        "                    encoded_tags.append(self.tag_vocab['<PAD>'])\n",
        "                else:\n",
        "                    encoded_tags.append(self.tag_vocab[tag_seq[word_idx]])\n",
        "            encoded_tags = torch.tensor(encoded_tags, dtype=torch.long)\n",
        "            encoded_data.append({'input_ids': input_ids, 'tags': encoded_tags})\n",
        "        return encoded_data\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.encoded_data)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        example = self.encoded_data[idx]\n",
        "        input_ids = example['input_ids']\n",
        "        tags = example['tags']\n",
        "        return input_ids, tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class POSTrainer:\n",
        "    def __init__(self, model: nn.Module, learning_rate: float, tag_pad_idx: int):\n",
        "        self.model = model\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.criterion = nn.CrossEntropyLoss(ignore_index=tag_pad_idx)\n",
        "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate)\n",
        "        print(f\"Using device: {self.device}\")\n",
        "\n",
        "    def train(self, dataset: Dataset, num_epochs: int = 5, batch_size: int = 32):\n",
        "        self.model.to(self.device)\n",
        "        self.model.train()\n",
        "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "        for epoch in range(num_epochs):\n",
        "            total_loss = 0.0\n",
        "            for input_ids, tags in dataloader:\n",
        "                input_ids, tags = input_ids.to(self.device), tags.to(self.device)\n",
        "                self.optimizer.zero_grad()\n",
        "                logits = self.model(input_ids) \n",
        "                loss = self.criterion(logits.view(-1, logits.shape[-1]), tags.view(-1))\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "                total_loss += loss.item()\n",
        "            print(f'Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(dataloader):.4f}')\n",
        "\n",
        "    def evaluate(self, dataset: POSTagDataset, batch_size: int = 32):\n",
        "        self.model.eval()\n",
        "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "        all_preds = []\n",
        "        all_tags = []\n",
        "        with torch.no_grad():\n",
        "            for input_ids, tags in dataloader:\n",
        "                input_ids, tags = input_ids.to(self.device), tags.to(self.device)\n",
        "                logits = self.model(input_ids)\n",
        "                preds = logits.argmax(dim=-1)\n",
        "                mask = tags != dataset.tag_vocab['<PAD>']\n",
        "                all_preds.extend(preds[mask].cpu().numpy())\n",
        "                all_tags.extend(tags[mask].cpu().numpy())\n",
        "        accuracy = accuracy_score(all_tags, all_preds)\n",
        "        print(f'POS Tagging Accuracy: {accuracy:.4f}')\n",
        "        tag_mapping = {idx: tag for tag, idx in dataset.tag_vocab.items() if tag != '<PAD>'}\n",
        "        labels = sorted(tag_mapping.keys())\n",
        "        target_names = [tag_mapping[idx] for idx in labels]\n",
        "        print(classification_report(all_tags, all_preds, labels=labels, target_names=target_names, zero_division=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Epoch 1/5, Loss: 0.9787\n",
            "Epoch 2/5, Loss: 0.4133\n",
            "Epoch 3/5, Loss: 0.3432\n",
            "Epoch 4/5, Loss: 0.3127\n",
            "Epoch 5/5, Loss: 0.2952\n",
            "POS Tagging Accuracy: 0.9239\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           #       1.00      1.00      1.00         9\n",
            "           $       1.00      1.00      1.00       378\n",
            "          ''       1.00      1.00      1.00       575\n",
            "           (       0.97      0.91      0.94       300\n",
            "           )       0.93      0.95      0.94       281\n",
            "           ,       0.97      1.00      0.99      2364\n",
            "           .       0.93      0.99      0.96      1830\n",
            "           :       0.99      0.99      0.99       342\n",
            "          CC       0.99      0.99      0.99      1192\n",
            "          CD       0.97      0.95      0.96      3195\n",
            "          DT       0.97      0.98      0.98      3954\n",
            "          EX       0.98      1.00      0.99        48\n",
            "          FW       0.00      0.00      0.00         4\n",
            "          IN       0.96      0.98      0.97      4973\n",
            "          JJ       0.78      0.78      0.78      4149\n",
            "         JJR       0.87      0.88      0.88       199\n",
            "         JJS       0.89      0.76      0.82        76\n",
            "          MD       1.00      0.96      0.98       472\n",
            "          NN       0.87      0.88      0.88      7332\n",
            "         NNP       0.92      0.92      0.92      7440\n",
            "        NNPS       0.72      0.40      0.52       161\n",
            "         NNS       0.94      0.92      0.93      3395\n",
            "         PDT       0.00      0.00      0.00        10\n",
            "         POS       0.99      0.99      0.99       825\n",
            "         PRP       0.99      0.98      0.98       803\n",
            "        PRP$       1.00      0.99      1.00       418\n",
            "          RB       0.88      0.86      0.87      1668\n",
            "         RBR       0.86      0.74      0.79        68\n",
            "         RBS       0.98      0.84      0.90        49\n",
            "          RP       0.50      0.25      0.33        12\n",
            "         SYM       0.00      0.00      0.00         0\n",
            "          TO       0.99      0.99      0.99      1163\n",
            "          UH       1.00      0.25      0.40         4\n",
            "          VB       0.93      0.92      0.92      1336\n",
            "         VBD       0.94      0.95      0.94      1705\n",
            "         VBG       0.90      0.77      0.83       847\n",
            "         VBN       0.82      0.85      0.84      1192\n",
            "         VBP       0.89      0.87      0.88       567\n",
            "         VBZ       0.97      0.96      0.96       974\n",
            "         WDT       0.94      0.91      0.93       194\n",
            "          WP       0.99      0.99      0.99       108\n",
            "         WP$       1.00      1.00      1.00         3\n",
            "         WRB       0.98      0.97      0.97        93\n",
            "          ``       1.00      1.00      1.00       634\n",
            "\n",
            "    accuracy                           0.92     55342\n",
            "   macro avg       0.87      0.83      0.84     55342\n",
            "weighted avg       0.92      0.92      0.92     55342\n",
            "\n"
          ]
        }
      ],
      "source": [
        "train_filepath = \"train_pos.txt\"\n",
        "test_filepath = \"test_pos.txt\"\n",
        "\n",
        "pos_train_dataset = POSTagDataset(train_filepath, max_len=50)\n",
        "pos_test_dataset = POSTagDataset(test_filepath, max_len=50, tag_vocab=pos_train_dataset.tag_vocab)\n",
        "\n",
        "tag_count = len(pos_train_dataset.tag_vocab)\n",
        "learning_rate = 0.001\n",
        "\n",
        "model = Transformer(tag_count)\n",
        "pos_trainer = POSTrainer(model, learning_rate, tag_pad_idx=pos_train_dataset.tag_vocab['<PAD>'])\n",
        "\n",
        "pos_trainer.train(pos_train_dataset, num_epochs=5, batch_size=32)\n",
        "pos_trainer.evaluate(pos_test_dataset, batch_size=32)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

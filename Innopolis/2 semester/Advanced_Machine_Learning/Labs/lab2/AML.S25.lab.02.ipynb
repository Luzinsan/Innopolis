{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqXoek6foX8O"
      },
      "source": [
        "# Lab 2: Generative Models (Generative adversarial networks)\n",
        "```\n",
        "- [S25] Advanced Machine Learning, Innopolis University\n",
        "- Teaching Assistant: Gcinizwe Dlamini\n",
        "```\n",
        "<hr>\n",
        "\n",
        "\n",
        "```\n",
        "Lab Plan\n",
        "1. Conditional Generative adversarial networks\n",
        "2. Bidirectional Generative Adversarial Network\n",
        "3. Task 2\n",
        "```\n",
        "\n",
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nnlrda5BqVSa"
      },
      "source": [
        "## 1. Conditional Generative adversarial network\n",
        "\n",
        "\n",
        "The Conditional Generative Adversarial Network (cGAN) is a model used in deep learning, a derivative of machine learning. It enables more precise generation and discrimination of data. The achitecture is similar to the one of vanilla GAN.\n",
        "\n",
        "\n",
        "The condition for data generation is a link to the label of the data.\n",
        "\n",
        "![Conditional GAN](https://www.researchgate.net/profile/Gerasimos-Spanakis/publication/330474693/figure/fig1/AS:956606955139072@1605084279074/GAN-conditional-GAN-CGAN-and-auxiliary-classifier-GAN-ACGAN-architectures-where-x_Q320.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4-1Z4q0sGU9"
      },
      "source": [
        "### Imports\n",
        "\n",
        "`!pip install tensorboardX`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HuMmBtBq0zRd",
        "outputId": "66c8eb8b-cf38-4c11-c120-07b0e9d9ae0e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision.datasets import ImageFolder, MNIST\n",
        "from torchvision import transforms\n",
        "from torch import autograd\n",
        "from torchvision.utils import make_grid\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJswv1OYsbB3"
      },
      "source": [
        "### 1.1 Dataset preparation\n",
        "\n",
        "In this task we will use MNIST dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "UBkjaMI0zxkz"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5], [0.5])\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "def load_dataset(batch_size = 128, root = '~/datasets/', transform=transforms.ToTensor()):\n",
        "    train_dataset = torchvision.datasets.MNIST(root = root + '/MNIST', train=True,\n",
        "                                               transform=transform, download=True)\n",
        "\n",
        "    test_dataset = torchvision.datasets.MNIST(root = root + '/MNIST', train=False,\n",
        "                                              transform=transform)\n",
        "\n",
        "    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
        "    return train_loader, test_loader\n",
        "\n",
        "train_loader, _ = load_dataset(batch_size=batch_size, transform=transform)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-JX3sESsmI3"
      },
      "source": [
        "### 1.2 Define Discriminator model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "3Mm02h1A1EeE"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    self.label_emb = nn.Embedding(10, 10)\n",
        "\n",
        "    self.model = nn.Sequential(\n",
        "        nn.Linear(794, 1024),\n",
        "        nn.LeakyReLU(0.2, inplace=True),\n",
        "        nn.Dropout(0.3),\n",
        "        nn.Linear(1024, 512),\n",
        "        nn.LeakyReLU(0.2, inplace=True),\n",
        "        nn.Dropout(0.3),\n",
        "        nn.Linear(512, 256),\n",
        "        nn.LeakyReLU(0.2, inplace=True),\n",
        "        nn.Dropout(0.3),\n",
        "        nn.Linear(256, 1),\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "\n",
        "  def forward(self, x, labels):\n",
        "    x = x.view(x.size(0), 784)\n",
        "    c = self.label_emb(labels)\n",
        "    x = torch.cat([x, c], 1)\n",
        "    out = self.model(x)\n",
        "    return out.squeeze()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXbUak2MsuE9"
      },
      "source": [
        "### 1.3 Define Generator model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "1DwfBnG11Mwg"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.label_emb = nn.Embedding(10, 10)\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(110, 256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(256, 512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(512, 1024),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(1024, 784),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, z, labels):\n",
        "        z = z.view(z.size(0), 100)\n",
        "        c = self.label_emb(labels)\n",
        "        x = torch.cat([z, c], 1)\n",
        "        out = self.model(x)\n",
        "        return out.view(x.size(0), 28, 28)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cfd2PhSFtAbj"
      },
      "source": [
        "### 1.4 Define Conditional GAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "DAPqOjb_1PzH"
      },
      "outputs": [],
      "source": [
        "generator = Generator().to(device)\n",
        "discriminator = Discriminator().to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3atg_a5tFcJ"
      },
      "source": [
        "### 1.5 Define Training params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "rJLE2Wdn1S-5"
      },
      "outputs": [],
      "source": [
        "criterion = nn.BCELoss()\n",
        "d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=1e-4)\n",
        "g_optimizer = torch.optim.Adam(generator.parameters(), lr=1e-4)\n",
        "\n",
        "writer = SummaryWriter('')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDSby137uJNh"
      },
      "source": [
        "### 1.6 Generator Training procedure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "_QXNYjy31Zks"
      },
      "outputs": [],
      "source": [
        "def generator_train_step(batch_size, discriminator, generator, g_optimizer, criterion, device='cpu'):\n",
        "\n",
        "  g_optimizer.zero_grad()\n",
        "  z = torch.randn(batch_size, 100).to(device)\n",
        "  fake_labels = torch.LongTensor(np.random.randint(0, 10, batch_size)).to(device)\n",
        "  fake_images = generator(z, fake_labels)\n",
        "  validity = discriminator(fake_images, fake_labels)\n",
        "  g_loss = criterion(validity, torch.ones(batch_size).to(device))\n",
        "  g_loss.backward()\n",
        "  g_optimizer.step()\n",
        "  return g_loss.item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NPJzWnQuQYG"
      },
      "source": [
        "### 1.7 Discriminator Training procedure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "EiJL-FFI1dSs"
      },
      "outputs": [],
      "source": [
        "def discriminator_train_step(batch_size, discriminator, generator, d_optimizer, criterion, real_images, labels, device='cpu'):\n",
        "    d_optimizer.zero_grad()\n",
        "\n",
        "    # train with real images\n",
        "    real_validity = discriminator(real_images, labels)\n",
        "    real_loss = criterion(real_validity, torch.ones(batch_size).to(device))\n",
        "\n",
        "    # train with fake images\n",
        "    z = torch.randn(batch_size, 100).to(device)\n",
        "    fake_labels = torch.LongTensor(np.random.randint(0, 10, batch_size)).to(device)\n",
        "    fake_images = generator(z, fake_labels)\n",
        "    fake_validity = discriminator(fake_images, fake_labels)\n",
        "    fake_loss = criterion(fake_validity, torch.zeros(batch_size).to(device))\n",
        "\n",
        "    d_loss = real_loss + fake_loss\n",
        "    d_loss.backward()\n",
        "    d_optimizer.step()\n",
        "    return d_loss.item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ni6YAZKRuW4O"
      },
      "source": [
        "### 1.8 Conditional GAN training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "JEMaBlXr1ic1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 ... Done!\n",
            "Epoch 1 ... Done!\n",
            "Epoch 2 ... Done!\n",
            "Epoch 3 ... Done!\n",
            "Epoch 4 ... Done!\n",
            "Epoch 5 ... Done!\n",
            "Epoch 6 ... Done!\n",
            "Epoch 7 ... Done!\n",
            "Epoch 8 ... Done!\n",
            "Epoch 9 ... Done!\n",
            "Epoch 10 ... Done!\n",
            "Epoch 11 ... Done!\n",
            "Epoch 12 ... Done!\n",
            "Epoch 13 ... Done!\n",
            "Epoch 14 ... Done!\n",
            "Epoch 15 ... Done!\n",
            "Epoch 16 ... Done!\n",
            "Epoch 17 ... Done!\n",
            "Epoch 18 ... Done!\n",
            "Epoch 19 ... Done!\n",
            "Epoch 20 ... Done!\n",
            "Epoch 21 ... Done!\n",
            "Epoch 22 ... Done!\n",
            "Epoch 23 ... Done!\n",
            "Epoch 24 ... Done!\n",
            "Epoch 25 ... Done!\n",
            "Epoch 26 ... Done!\n",
            "Epoch 27 ... Done!\n",
            "Epoch 28 ... Done!\n",
            "Epoch 29 ... Done!\n",
            "Epoch 30 ... Done!\n",
            "Epoch 31 ... Done!\n",
            "Epoch 32 ... Done!\n",
            "Epoch 33 ... Done!\n",
            "Epoch 34 ... Done!\n",
            "Epoch 35 ... Done!\n",
            "Epoch 36 ... Done!\n",
            "Epoch 37 ... Done!\n",
            "Epoch 38 ... Done!\n",
            "Epoch 39 ... Done!\n",
            "Epoch 40 ... Done!\n",
            "Epoch 41 ... Done!\n",
            "Epoch 42 ... Done!\n",
            "Epoch 43 ... Done!\n",
            "Epoch 44 ... Done!\n",
            "Epoch 45 ... Done!\n",
            "Epoch 46 ... Done!\n",
            "Epoch 47 ... Done!\n",
            "Epoch 48 ... Done!\n",
            "Epoch 49 ... Done!\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 50\n",
        "n_critic = 5\n",
        "display_step = 10\n",
        "for epoch in range(num_epochs):\n",
        "    print('Epoch {} ...'.format(epoch), end=' ')\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "\n",
        "        step = epoch * len(train_loader) + i + 1\n",
        "        real_images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        generator.train()\n",
        "\n",
        "        d_loss = 0\n",
        "        for _ in range(n_critic):\n",
        "            d_loss = discriminator_train_step(len(real_images), discriminator,\n",
        "                                              generator, d_optimizer, criterion,\n",
        "                                              real_images, labels, device=device)\n",
        "\n",
        "\n",
        "        g_loss = generator_train_step(batch_size, discriminator, generator, g_optimizer, criterion, device=device)\n",
        "\n",
        "        writer.add_scalars('scalars', {'g_loss': g_loss, 'd_loss': (d_loss / n_critic)}, step)\n",
        "\n",
        "        if step % display_step == 0:\n",
        "            generator.eval()\n",
        "            z = torch.randn(9, 100).to(device)\n",
        "            labels = torch.LongTensor(np.arange(9)).to(device)\n",
        "            sample_images = generator(z, labels).unsqueeze(1)\n",
        "            grid = make_grid(sample_images, nrow=3, normalize=True)\n",
        "            writer.add_image('sample_image', grid, step)\n",
        "    print('Done!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fR7Cavruu7Qs"
      },
      "source": [
        "### 1.9 Generate Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-fTIWhWDIZpS"
      },
      "outputs": [],
      "source": [
        "def generate_digit(generator, digit):\n",
        "    z = torch.randn(1, 100).to(device)\n",
        "    label = torch.LongTensor([digit]).to(device)\n",
        "    img = generator(z, label).data.cpu()\n",
        "    img = 0.5 * img + 0.5\n",
        "    return transforms.ToPILImage()(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "1VKZyz-Az0yj"
      },
      "outputs": [
        {
          "data": {
            "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APC9L0u+1vVLfTdNtnuby4fZFEnVj/IADJJPAAJPAr1FfgjaafZwjxN450jRdTlj8z7FKUYqD0yxkXPOQcAjIOCa4bxb4H1jwZLbf2iLeW2u13W13ay+ZFMMAnaeD/EOoHtkVzdX9G1i80HU01Gwk8u6jV1jkBIKbkKkggjkBj/9fpXWeD/At74yuJtd12/ax0KOQyX2qXL/ADSHPzBWb7zEnGT0J7ng3Pin46tNfksfDvh8lfDekxpHbZzmVlXaGO7nAHyjPPUnrx5xRX0XL4Q8P+PPh1pEHh3xZPp+nQOsZtr4hlaVVw24ZB8zBz1K4+6AK5pfgXplncNFrHj7SbV0BcxgLu2jqTucYx3ODivHJAiyMIySo6E0sEzW1xFOgjLxuHUSRq6kg55VgQw9iCD3qOiiv//Z",
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABIUlEQVR4AWNgGFDQ+/fPvy+HmbC5QeUfGPz59UiDGV3+J0jq14/vv37++b0ZVZLxLVDqoyoTIwu3+/t//0xQZBm//PsrBxV5/e/fcxRJzt5wVpjA33//mmFsEM3Lt4cPxr/6799WGBtCMzNC+Yzv//3+hSoH53G8+ffvrxmcC2EwQXT6PPnz7+8kNDkIl7n49sffC7V5sEmydHz79dUObjuyEnaGkmV/f5vDHIYsxcDAyCCy82wWdjkGBo4/u6UkUDUgeOf+/fu9iR3K51IIVAcy4QbdUAPy3q3SP/ZAWcJcgvO3uMEBqEogpbANFGkg8AdMiiKkgCyWWWBBKPEDZD/cWAYG6UOf50uquWz8u/yQl1rXXxSdODmMDL9xypElAQA77oBSeppPiQAAAABJRU5ErkJggg==",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=28x28>"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generator.eval()\n",
        "generate_digit(generator, 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eo1zeMgnupe3"
      },
      "source": [
        "## 2. Structure of a Bidirectional Generative Adversarial Network (BiGAN)\n",
        "A BiGAN, or Bidirectional GAN, is a type of generative adversarial network where the generator not only maps latent samples to generated data, but also has an inverse mapping from data to the latent representation.\n",
        "![](https://ar5iv.labs.arxiv.org/html/1801.04271/assets/bigan.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Qv-heWe4qX18"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HcUi0Nb190x"
      },
      "source": [
        "### 2.1 Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "x8ha0RSC9kfj"
      },
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "mnist_train, mnist_test = load_dataset(batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcTI4Hm72YXY"
      },
      "source": [
        "### 2.2 Define Generator\n",
        "\n",
        "- **Role:** Takes a latent vector \\( z \\) (sampled from a simple prior, like a Gaussian) and generates a synthetic data sample \\( G(z) \\) (e.g., an image).\n",
        "- **Goal:** Produce realistic outputs so that the discriminator cannot distinguish them from real data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "7dQYkT_09ugA"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(50, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, 1024),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, 28 * 28),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        return self.layers(z)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45ZNAgll2jqE"
      },
      "source": [
        "### 2.3 Define Discriminator\n",
        "\n",
        "- **Role:** Receives pairs of data and latent codes. It sees:\n",
        "  - **Real pair:** \\((x, E(x))\\) where \\( x \\) is a real data sample and \\( E(x) \\) is its encoded latent representation.\n",
        "  - **Fake pair:** \\((G(z), z)\\) where \\( z \\) is a sampled latent vector and \\( G(z) \\) is the generated data sample.\n",
        "- **Goal:** Distinguish between real and fake pairs by outputting a probability that a given pair is real.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "KZa_8zL09yN5"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(28 * 28 + 50, 1024),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(1024, 1024),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(1024,1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, X, z):\n",
        "        Xz = torch.cat([X, z], dim=1)\n",
        "        return self.layers(Xz)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyKmRtQn2dlp"
      },
      "source": [
        "### 2.4 Define Encoder\n",
        "\n",
        "- **Role:** Maps a real data sample \\( x \\) (e.g., an image) to a latent representation \\( E(x) \\).\n",
        "- **Goal:** Learn an inverse mapping of the generator, so that the latent code encapsulates meaningful information about the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "AZseAzle9rh8"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(28 * 28, 1024),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(1024, 1024),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(1024, 50)\n",
        "        )\n",
        "\n",
        "    def forward(self, X):\n",
        "        return self.layers(X)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fgz7eABQ2u-m"
      },
      "source": [
        "### 2.5 Define Loss function for discriminator and Encoder-Generator\n",
        "\n",
        "4. **Discriminator Training:**\n",
        "   - **Objective:** Update $( D )$ so that it assigns a high probability (close to 1) to the real pair and a low probability (close to 0) to the fake pair.\n",
        "\n",
        "\n",
        "5. **Generator and Encoder Training:**\n",
        "   - **Objective:** Update $( G )$ and $( E )$ together so that the fake pairs $((G(z), z))$ become more similar to the real pairs $((x, E(x)))$, effectively “fooling” $(D)$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Y-80qP4x92tI"
      },
      "outputs": [],
      "source": [
        "def D_loss(DG, DE, eps=1e-6):\n",
        "    loss = torch.log(DE + eps) + torch.log(1 - DG + eps)\n",
        "    return -torch.mean(loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "bViWciZo9_uz"
      },
      "outputs": [],
      "source": [
        "def EG_loss(DG, DE, eps=1e-6):\n",
        "    loss = torch.log(DG + eps) + torch.log(1 - DE + eps)\n",
        "    return -torch.mean(loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def init_weights(Layer):\n",
        "    name = Layer.__class__.__name__\n",
        "    if name == 'Linear':\n",
        "        torch.nn.init.normal_(Layer.weight, mean=0, std=0.02)\n",
        "        if Layer.bias is not None:\n",
        "            torch.nn.init.constant_(Layer.bias, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzhwHxH63Bj7"
      },
      "source": [
        "### 2.6 Define BiGAN and Training Params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "nde4eOHf-HAV"
      },
      "outputs": [],
      "source": [
        "l_rate = 2e-5\n",
        "\n",
        "E = Encoder().to(device)\n",
        "G = Generator().to(device)\n",
        "D = Discriminator().to(device)\n",
        "\n",
        "E.apply(init_weights)\n",
        "G.apply(init_weights)\n",
        "D.apply(init_weights)\n",
        "\n",
        "#optimizers with weight decay\n",
        "optimizer_EG = torch.optim.Adam(list(E.parameters()) + list(G.parameters()),\n",
        "                                lr=l_rate, betas=(0.5, 0.999), weight_decay=1e-5)\n",
        "optimizer_D = torch.optim.Adam(D.parameters(),\n",
        "                               lr=l_rate, betas=(0.5, 0.999), weight_decay=1e-5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWQHnA4Iz5Am"
      },
      "source": [
        "## 3. Task 2\n",
        "```\n",
        "Task 2.1\n",
        "- Train defined BiGAN above\n",
        "- In the training procedure add tensorboard and every 10 epochs visualize 10\n",
        "  - generated images\n",
        "  - reconstructed images\n",
        "```\n",
        "<hr>\n",
        "\n",
        "```\n",
        "Task 2.2\n",
        "- Implement and train a conditional BiGAN for CIFAR10 dataset\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 2.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "RT3IIvrqa8sT"
      },
      "outputs": [],
      "source": [
        "writer = SummaryWriter('', filename_suffix='_task2.1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generator_encoder_train_step(discriminator, generator, encoder, real_images, g_optimizer, device='cpu'):\n",
        "\n",
        "  g_optimizer.zero_grad()\n",
        "  z = 2 * torch.rand(real_images.size(0), 50) - 1\n",
        "  z = z.to(device)\n",
        "  fake_images = generator(z)\n",
        "  latent_vector = encoder(real_images)\n",
        "\n",
        "  DG = discriminator(fake_images, z)\n",
        "  DE = discriminator(real_images, latent_vector)\n",
        "\n",
        "  loss_EG = EG_loss(DG, DE)\n",
        "\n",
        "  loss_EG.backward()\n",
        "  g_optimizer.step()\n",
        "  return loss_EG.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "def discriminator_train_step(discriminator, generator, encoder, d_optimizer, real_images, device='cpu'):\n",
        "    d_optimizer.zero_grad()\n",
        "\n",
        "    # train with real images\n",
        "    latent_vector = encoder(real_images)\n",
        "    DE = discriminator(real_images, latent_vector)\n",
        "\n",
        "    # train with fake images\n",
        "    z = 2 * torch.rand(real_images.size(0), 50) - 1\n",
        "    z = z.to(device)\n",
        "    fake_images = generator(z)\n",
        "    DG = discriminator(fake_images, z)\n",
        "    \n",
        "    #compute losses\n",
        "    loss_D = D_loss(DG, DE)\n",
        "   \n",
        "    loss_D.backward()\n",
        "    d_optimizer.step()\n",
        "    return loss_D.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/30], Avg_Loss_D: 0.2564, Avg_Loss_EG: 4.3752\n",
            "Done!\n",
            "Epoch [2/30], Avg_Loss_D: 0.0914, Avg_Loss_EG: 6.7360\n",
            "Done!\n",
            "Epoch [3/30], Avg_Loss_D: 0.0986, Avg_Loss_EG: 6.5528\n",
            "Done!\n",
            "Epoch [4/30], Avg_Loss_D: 0.0930, Avg_Loss_EG: 7.2560\n",
            "Done!\n",
            "Epoch [5/30], Avg_Loss_D: 0.0497, Avg_Loss_EG: 9.0924\n",
            "Done!\n",
            "Epoch [6/30], Avg_Loss_D: 0.2029, Avg_Loss_EG: 5.9365\n",
            "Done!\n",
            "Epoch [7/30], Avg_Loss_D: 0.0995, Avg_Loss_EG: 6.8211\n",
            "Done!\n",
            "Epoch [8/30], Avg_Loss_D: 0.0325, Avg_Loss_EG: 9.0642\n",
            "Done!\n",
            "Epoch [9/30], Avg_Loss_D: 0.0771, Avg_Loss_EG: 7.9964\n",
            "Done!\n",
            "Epoch [10/30], Avg_Loss_D: 0.0440, Avg_Loss_EG: 9.0893\n",
            "Done!\n",
            "Epoch [11/30], Avg_Loss_D: 0.0412, Avg_Loss_EG: 8.6907\n",
            "Done!\n",
            "Epoch [12/30], Avg_Loss_D: 0.0227, Avg_Loss_EG: 11.5688\n",
            "Done!\n",
            "Epoch [13/30], Avg_Loss_D: 0.1460, Avg_Loss_EG: 7.5683\n",
            "Done!\n",
            "Epoch [14/30], Avg_Loss_D: 0.0533, Avg_Loss_EG: 8.7658\n",
            "Done!\n",
            "Epoch [15/30], Avg_Loss_D: 0.0422, Avg_Loss_EG: 8.9357\n",
            "Done!\n",
            "Epoch [16/30], Avg_Loss_D: 0.0543, Avg_Loss_EG: 9.7516\n",
            "Done!\n",
            "Epoch [17/30], Avg_Loss_D: 0.0918, Avg_Loss_EG: 8.5959\n",
            "Done!\n",
            "Epoch [18/30], Avg_Loss_D: 0.0558, Avg_Loss_EG: 9.1944\n",
            "Done!\n",
            "Epoch [19/30], Avg_Loss_D: 0.0221, Avg_Loss_EG: 10.7637\n",
            "Done!\n",
            "Epoch [20/30], Avg_Loss_D: 0.0280, Avg_Loss_EG: 9.6893\n",
            "Done!\n",
            "Epoch [21/30], Avg_Loss_D: 0.0502, Avg_Loss_EG: 8.3839\n",
            "Done!\n",
            "Epoch [22/30], Avg_Loss_D: 0.0441, Avg_Loss_EG: 9.2867\n",
            "Done!\n",
            "Epoch [23/30], Avg_Loss_D: 0.0384, Avg_Loss_EG: 11.3699\n",
            "Done!\n",
            "Epoch [24/30], Avg_Loss_D: 0.0601, Avg_Loss_EG: 8.5527\n",
            "Done!\n",
            "Epoch [25/30], Avg_Loss_D: 0.0442, Avg_Loss_EG: 9.5449\n",
            "Done!\n",
            "Epoch [26/30], Avg_Loss_D: 0.0251, Avg_Loss_EG: 10.1927\n",
            "Done!\n",
            "Epoch [27/30], Avg_Loss_D: 0.0545, Avg_Loss_EG: 8.0946\n",
            "Done!\n",
            "Epoch [28/30], Avg_Loss_D: 0.2594, Avg_Loss_EG: 7.3490\n",
            "Done!\n",
            "Epoch [29/30], Avg_Loss_D: 0.0690, Avg_Loss_EG: 9.3343\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 30\n",
        "n_critic = 5\n",
        "display_step = 1\n",
        "\n",
        "for epoch in range(1, num_epochs):\n",
        "    D.train()\n",
        "    E.train()\n",
        "    G.train()\n",
        "    \n",
        "    for i, (images, _) in enumerate(mnist_train):\n",
        "        step = (epoch - 1) * len(mnist_train) + i + 1\n",
        "        real_images = images.reshape(images.size(0),-1).to(device)\n",
        "        eg_loss = generator_encoder_train_step(D, G, E, real_images, optimizer_EG, device=device)\n",
        "        loss_D = 0\n",
        "        for _ in range(n_critic):\n",
        "            loss_D += discriminator_train_step(D, G, E,\n",
        "                                              optimizer_D,\n",
        "                                              real_images, device=device)\n",
        "        d_loss = loss_D / n_critic\n",
        "\n",
        "        \n",
        "    if epoch % display_step == 0:\n",
        "        print(f'Epoch [{epoch}/{num_epochs}], Avg_Loss_D: {d_loss:.4f}, Avg_Loss_EG: {eg_loss:.4f}')\n",
        "        writer.add_scalars('scalars', {'Avg_Loss_EG': eg_loss, 'Avg_Loss_D': d_loss}, epoch)\n",
        "\n",
        "        n_show = 10\n",
        "        D.eval()\n",
        "        E.eval()\n",
        "        G.eval()\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            real = images[:n_show].reshape(n_show,-1).to(device)\n",
        "            z = 2 * torch.rand(n_show, 50) - 1\n",
        "            z = z.to(device)\n",
        "            gener = G(z).reshape(n_show, 28, 28).cpu().numpy()\n",
        "            recon = G(E(real)).reshape(n_show, 28, 28).cpu().numpy()\n",
        "            real = real.reshape(n_show, 28, 28).cpu().numpy()\n",
        "\n",
        "            fig, ax = plt.subplots(3, n_show, figsize=(15,5))\n",
        "            fig.subplots_adjust(wspace=0.05, hspace=0)\n",
        "            plt.rcParams.update({'font.size': 20})\n",
        "            fig.suptitle(f'Epoch {epoch}')\n",
        "            fig.text(0.04, 0.75, 'G(z)', ha='left')\n",
        "            fig.text(0.04, 0.5, 'x', ha='left')\n",
        "            fig.text(0.04, 0.25, 'G(E(x))', ha='left')\n",
        "\n",
        "            for i in range(n_show):\n",
        "                ax[0, i].imshow(gener[i], cmap='gray')\n",
        "                ax[0, i].axis('off')\n",
        "                ax[1, i].imshow(real[i], cmap='gray')\n",
        "                ax[1, i].axis('off')\n",
        "                ax[2, i].imshow(recon[i], cmap='gray')\n",
        "                ax[2, i].axis('off')\n",
        "            \n",
        "            writer.add_figure('sample_image', fig, step)\n",
        "    print('Done!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 2.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, latent_dim=50, embedding_dim=10):\n",
        "        super(Generator, self).__init__()\n",
        "        self.label_emb = nn.Embedding(10, embedding_dim)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(latent_dim + embedding_dim, 256 * 4 * 4),\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "        self.deconv = nn.Sequential(\n",
        "            # Input: (256, 4, 4) -> output: (128, 8, 8)\n",
        "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(True),\n",
        "            # (128, 8, 8) -> (64, 16, 16)\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(True),\n",
        "            # (64, 16, 16) -> (3, 32, 32)\n",
        "            nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "        \n",
        "    def forward(self, z, labels):\n",
        "        # z: (batch, latent_dim); labels: (batch,)\n",
        "        e = self.label_emb(labels)               # (batch, embedding_dim)\n",
        "        x = torch.cat([z, e], dim=1)             # (batch, latent_dim + embedding_dim)\n",
        "        x = self.fc(x)                           # (batch, 256*4*4)\n",
        "        x = x.view(x.size(0), 256, 4, 4)         # (batch, 256, 4, 4)\n",
        "        x = self.deconv(x)                       # (batch, 3, 32, 32)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, embedding_dim=10):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.label_emb = nn.Embedding(10, embedding_dim)\n",
        "        \n",
        "        self.label_fc = nn.Linear(embedding_dim, 32 * 32)\n",
        "        \n",
        "        self.conv = nn.Sequential(\n",
        "            # Input: (3 channels) + label (1 channel)\n",
        "            nn.Conv2d(4, 64, kernel_size=4, stride=2, padding=1, bias=False),  # (64, 16, 16)\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1, bias=False), # (128, 8, 8)\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1, bias=False), # (256, 4, 4)\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2, inplace=True)\n",
        "        )\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(256 * 4 * 4, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        \n",
        "    def forward(self, X, z, labels):\n",
        "        # X: (batch, 3, 32, 32); labels: (batch,)\n",
        "        batch_size = X.size(0)\n",
        "        e = self.label_emb(labels)             # (batch, embedding_dim)\n",
        "        e = self.label_fc(e)                   # (batch, 32*32)\n",
        "        e = e.view(batch_size, 1, 32, 32)      # (batch, 1, 32, 32)\n",
        "        x_cond = torch.cat([X, e], dim=1)      # (batch, 3+1=4, 32, 32)\n",
        "        out = self.conv(x_cond)\n",
        "        out = out.view(batch_size, -1)\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, embedding_dim=10):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.label_emb = nn.Embedding(10, embedding_dim)\n",
        "        self.label_fc = nn.Linear(embedding_dim, 32 * 32)\n",
        "        \n",
        "        self.conv = nn.Sequential(\n",
        "            # Input: (3 channels) + label (1 channel)\n",
        "            nn.Conv2d(4, 64, kernel_size=4, stride=2, padding=1),  # (64, 16, 16)\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1), # (128, 8, 8)\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1), # (256, 4, 4)\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2, inplace=True)\n",
        "        )\n",
        "        self.fc = nn.Linear(256 * 4 * 4, 50)\n",
        "        \n",
        "    def forward(self, X, labels):\n",
        "        batch_size = X.size(0)\n",
        "        e = self.label_emb(labels)              # (batch, embedding_dim)\n",
        "        e = self.label_fc(e)                    # (batch, 32*32)\n",
        "        e = e.view(batch_size, 1, 32, 32)       # (batch, 1, 32, 32)\n",
        "        x_cond = torch.cat([X, e], dim=1)       # (batch, 4, 32, 32)\n",
        "        out = self.conv(x_cond)\n",
        "        out = out.view(batch_size, -1)\n",
        "        latent = self.fc(out)\n",
        "        return latent\n",
        "\n",
        "    \n",
        "\n",
        "def discriminator_train_step(discriminator, generator, encoder, d_optimizer, real_images, real_labels, device='cpu'):\n",
        "    d_optimizer.zero_grad()\n",
        "\n",
        "    # train with real images\n",
        "    latent_vector = encoder(real_images, real_labels)\n",
        "    DE = discriminator(real_images, latent_vector, real_labels)\n",
        "\n",
        "    # train with fake images\n",
        "    z = 2 * torch.rand(images.size(0), 50) - 1\n",
        "    z = z.to(device)\n",
        "    fake_labels = torch.LongTensor(np.random.randint(0, 10, real_images.size(0))).to(device)\n",
        "    fake_images = generator(z, fake_labels)\n",
        "    DG = discriminator(fake_images, z, fake_labels)\n",
        "    \n",
        "    #compute losses\n",
        "    loss_D = D_loss(DG, DE)\n",
        "   \n",
        "    loss_D.backward()\n",
        "    d_optimizer.step()\n",
        "    return loss_D.item()\n",
        "\n",
        "\n",
        "def generator_encoder_train_step(discriminator, generator, encoder, real_images, real_labels, g_optimizer, device='cpu'):\n",
        "\n",
        "    g_optimizer.zero_grad()\n",
        "    z = 2 * torch.rand(real_images.size(0), 50) - 1\n",
        "    z = z.to(device)\n",
        "    fake_labels = torch.LongTensor(np.random.randint(0, 10, real_images.size(0))).to(device)\n",
        "    fake_images = generator(z, fake_labels)\n",
        "    latent_vector = encoder(real_images, real_labels)\n",
        "\n",
        "    DG = discriminator(fake_images, z, fake_labels)\n",
        "    DE = discriminator(real_images, latent_vector, real_labels)\n",
        "\n",
        "    loss_EG = EG_loss(DG, DE)\n",
        "\n",
        "    loss_EG.backward()\n",
        "    g_optimizer.step()\n",
        "    return loss_EG.item()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "l_rate = 2e-5\n",
        "\n",
        "E = Encoder().to(device)\n",
        "G = Generator().to(device)\n",
        "D = Discriminator().to(device)\n",
        "\n",
        "E.apply(init_weights)\n",
        "G.apply(init_weights)\n",
        "D.apply(init_weights)\n",
        "\n",
        "#optimizers with weight decay\n",
        "optimizer_EG = torch.optim.Adam(list(E.parameters()) + list(G.parameters()),\n",
        "                                lr=l_rate, betas=(0.5, 0.999), weight_decay=1e-5)\n",
        "optimizer_D = torch.optim.Adam(D.parameters(),\n",
        "                               lr=l_rate, betas=(0.5, 0.999), weight_decay=1e-5)\n",
        "\n",
        "writer = SummaryWriter('', filename_suffix='_task2.2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5], [0.5])\n",
        "])\n",
        "\n",
        "\n",
        "def load_dataset(batch_size = 128, root = '~/datasets/', transform=transforms.ToTensor()):\n",
        "    train_dataset = torchvision.datasets.CIFAR10(root = root + '/CIFAR10', train=True,\n",
        "                                               transform=transform, download=True)\n",
        "\n",
        "    test_dataset = torchvision.datasets.CIFAR10(root = root + '/CIFAR10', train=False,\n",
        "                                              transform=transform)\n",
        "\n",
        "    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
        "    return train_loader, test_loader\n",
        "\n",
        "cifar_loader, _ = load_dataset(batch_size=batch_size, transform=transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/30], Avg_Loss_D: 0.0043, Avg_Loss_EG: 14.7523\n",
            "Done!\n",
            "Epoch [2/30], Avg_Loss_D: 0.0022, Avg_Loss_EG: 17.7251\n",
            "Done!\n",
            "Epoch [3/30], Avg_Loss_D: 0.0021, Avg_Loss_EG: 19.5404\n",
            "Done!\n",
            "Epoch [4/30], Avg_Loss_D: 0.0033, Avg_Loss_EG: 14.7663\n",
            "Done!\n",
            "Epoch [5/30], Avg_Loss_D: 0.0005, Avg_Loss_EG: 19.7046\n",
            "Done!\n",
            "Epoch [6/30], Avg_Loss_D: 0.0057, Avg_Loss_EG: 14.9596\n",
            "Done!\n",
            "Epoch [7/30], Avg_Loss_D: 0.0031, Avg_Loss_EG: 18.3187\n",
            "Done!\n",
            "Epoch [8/30], Avg_Loss_D: 0.0011, Avg_Loss_EG: 17.8246\n",
            "Done!\n",
            "Epoch [9/30], Avg_Loss_D: 0.0031, Avg_Loss_EG: 14.4047\n",
            "Done!\n",
            "Epoch [10/30], Avg_Loss_D: 0.0088, Avg_Loss_EG: 13.1464\n",
            "Done!\n",
            "Epoch [11/30], Avg_Loss_D: 0.0032, Avg_Loss_EG: 15.7511\n",
            "Done!\n",
            "Epoch [12/30], Avg_Loss_D: 0.0057, Avg_Loss_EG: 14.6656\n",
            "Done!\n",
            "Epoch [13/30], Avg_Loss_D: 0.0080, Avg_Loss_EG: 14.2493\n",
            "Done!\n",
            "Epoch [14/30], Avg_Loss_D: 0.0051, Avg_Loss_EG: 14.6086\n",
            "Done!\n",
            "Epoch [15/30], Avg_Loss_D: 0.0092, Avg_Loss_EG: 13.9169\n",
            "Done!\n",
            "Epoch [16/30], Avg_Loss_D: 0.0124, Avg_Loss_EG: 14.7024\n",
            "Done!\n",
            "Epoch [17/30], Avg_Loss_D: 0.0041, Avg_Loss_EG: 14.7642\n",
            "Done!\n",
            "Epoch [18/30], Avg_Loss_D: 0.0058, Avg_Loss_EG: 16.5325\n",
            "Done!\n",
            "Epoch [19/30], Avg_Loss_D: 0.0065, Avg_Loss_EG: 18.3034\n",
            "Done!\n",
            "Epoch [20/30], Avg_Loss_D: 0.0055, Avg_Loss_EG: 16.4234\n",
            "Done!\n",
            "Epoch [21/30], Avg_Loss_D: 0.0072, Avg_Loss_EG: 14.2589\n",
            "Done!\n",
            "Epoch [22/30], Avg_Loss_D: 0.0065, Avg_Loss_EG: 15.2649\n",
            "Done!\n",
            "Epoch [23/30], Avg_Loss_D: 0.0072, Avg_Loss_EG: 14.9635\n",
            "Done!\n",
            "Epoch [24/30], Avg_Loss_D: 0.0129, Avg_Loss_EG: 13.4041\n",
            "Done!\n",
            "Epoch [25/30], Avg_Loss_D: 0.0031, Avg_Loss_EG: 16.8310\n",
            "Done!\n",
            "Epoch [26/30], Avg_Loss_D: 0.0049, Avg_Loss_EG: 21.8291\n",
            "Done!\n",
            "Epoch [27/30], Avg_Loss_D: 0.0023, Avg_Loss_EG: 18.1911\n",
            "Done!\n",
            "Epoch [28/30], Avg_Loss_D: 0.0025, Avg_Loss_EG: 15.9173\n",
            "Done!\n",
            "Epoch [29/30], Avg_Loss_D: 0.0027, Avg_Loss_EG: 19.4096\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 30\n",
        "n_critic = 5\n",
        "n_generate = 1\n",
        "display_step = 1\n",
        "\n",
        "for epoch in range(1, num_epochs):\n",
        "    D.train()\n",
        "    E.train()\n",
        "    G.train()\n",
        "    \n",
        "    for i, (images, labels) in enumerate(cifar_loader):\n",
        "        step = (epoch - 1) * len(cifar_loader) + i + 1\n",
        "        real_images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        loss_D = 0\n",
        "        for _ in range(n_critic):\n",
        "            loss_D += discriminator_train_step(D, G, E,\n",
        "                                              optimizer_D,\n",
        "                                              real_images, labels, device=device)\n",
        "        d_loss = loss_D / n_critic\n",
        "            \n",
        "        eg_loss = 0\n",
        "        for _ in range(n_generate):\n",
        "            eg_loss += generator_encoder_train_step(D, G, E, real_images, labels, optimizer_EG, device=device)\n",
        "        eg_loss = eg_loss / n_generate\n",
        "\n",
        "        \n",
        "    if epoch % display_step == 0:\n",
        "        print(f'Epoch [{epoch}/{num_epochs}], Avg_Loss_D: {d_loss:.4f}, Avg_Loss_EG: {eg_loss:.4f}')\n",
        "        writer.add_scalars('scalars', {'Avg_Loss_EG': eg_loss, 'Avg_Loss_D': d_loss}, epoch)\n",
        "\n",
        "        n_show = 10\n",
        "        D.eval()\n",
        "        E.eval()\n",
        "        G.eval()\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            real = images[:n_show].to(device)\n",
        "            real_labels = labels[:n_show].to(device)\n",
        "            z = 2 * torch.rand(n_show, 50) - 1\n",
        "            z = z.to(device)\n",
        "            fake_labels_vis = torch.randint(0, 10, (n_show,), device=device)\n",
        "            gener = G(z, fake_labels_vis).reshape(n_show, 3, 32, 32).permute(0 ,2,3,1).cpu().numpy()\n",
        "            recon = G(E(real, real_labels), real_labels).reshape(n_show, 3, 32, 32).permute(0,2,3,1).cpu().numpy()\n",
        "            real = real.reshape(n_show, 3, 32, 32).permute(0,2,3,1).cpu().numpy()\n",
        "\n",
        "            normalize = lambda x: (x - x.min()) / (x.max() - x.min() + 1e-8)\n",
        "            gener = normalize(gener)\n",
        "            recon = normalize(recon)\n",
        "            real = normalize(real)\n",
        "\n",
        "            fig, ax = plt.subplots(3, n_show, figsize=(15,5))\n",
        "            fig.subplots_adjust(wspace=0.05, hspace=0)\n",
        "            plt.rcParams.update({'font.size': 20})\n",
        "            fig.suptitle(f'Epoch {epoch}')\n",
        "            fig.text(0.04, 0.75, 'G(z)', ha='left')\n",
        "            fig.text(0.04, 0.5, 'x', ha='left')\n",
        "            fig.text(0.04, 0.25, 'G(E(x))', ha='left')\n",
        "\n",
        "            for i in range(n_show):\n",
        "                ax[0, i].imshow(gener[i])\n",
        "                ax[0, i].axis('off')\n",
        "                ax[1, i].imshow(real[i])\n",
        "                ax[1, i].axis('off')\n",
        "                ax[2, i].imshow(recon[i])\n",
        "                ax[2, i].axis('off')\n",
        "            \n",
        "            writer.add_figure('sample_image', fig, step)\n",
        "    print('Done!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZsstGCzwb7a"
      },
      "source": [
        "## Resources\n",
        "\n",
        "* [Simple Explaination of BiGAN](https://youtu.be/rzpA0H-q_HY)\n",
        "* [Adversarial feature learning](https://arxiv.org/pdf/1605.09782v7.pdf) -- original BiGAN paper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0Q27eaJwky9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

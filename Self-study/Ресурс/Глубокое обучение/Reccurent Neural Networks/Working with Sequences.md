- В отличие от tabular данных с их фиксированными размерами, подпоследовательности представляются в виде упорядоченных векторов $x_1, ..., x_t, ..., x_T$, где t - индекс временно'го шага.
- В таких данных мы предполагаем, что существует взаимосвязь с временным шагом, рассматриваемым в этот момент в данных.
- Есть разновидности задач с последовательностями:
	1. sequential input -> fixed target
	2. fixed input -> sequential target (подпись к изображению)
	3. sequential input -> sequential target
		- aligned = однозначное сопоставление (выделение частей речи)
		- unaligned = вольное сопоставление (машинный перевод)

# Unsupervised density modeling - sequence modeling
## Autoregressive Models
- модели, которые предсказывают значение сигнала на основе предыдущих значений того же сигнала.
- Проблема: кол-во входов $x_t, ..., x_1$ варьируется, увеличивается по мере увеличения кол-ва данных, и их структура может модифицироваться (увеличиваться кол-во признаков)
- Решение: рассмотрение только окна данных размером $\tau$ и использование $x_t, .., x_{t-\tau}$ наблюдений; обрабатывать и обновлять сводку прошлых наблюдений $h_t$ в дополнении к предсказанию $\hat x_t$. Т.к. h_t никогда не измеряется напрямую, такие модели еще называют latent autoregressive models. ![[Latent autoregressive model.png]]
## Sequence Models
- оценка совместного распределения вероятностей (joint probability) всей последовательности, составленной из токенов (часто, слов, language model).
- Sequence models можно привести к авторегрессионной проблеме, декомпозируя совместные вероятности последовательности в произведение условных плотностей с помощью цепного правила: $$P(x_1, ..., x_T) = P(x_1)\prod_{t=2}^TP(x_t|x_{t-1},...,x_1)$$
- Рассматривая токены как слова, авторегрессионная модель должна быть вероятностным классификатором, предсказывая распределения вероятностей для следующего слова по всему словарю, зная предыдущий контекст.
### Markov Models
- Рассматривая лишь $\tau$ предыдущих шагов, и отбрасывая все остальное, чтобы не потерять в качестве предсказаний, последовательность должна удовлетворять **условию Маркова**: *будущее является условно независимым от прошлого, с точки зрения недавних событий*.
- Если $\tau=1$ ($\tau=k$ -- k-го порядка), то данные характеризуются марковской моделью первого порядка. Факторизация совместной вероятности становится произведением вероятностей для каждого слова в контекте предыдущего слова: $$P(x_1, ..., x_T) = P(x_1)\prod_{t=2}^TP(x_t|x_{t-1})$$
- Для практического использования достаточно приблизительного выполнения этого условия.
- Очень часто марковская модель просто подсчитывает встречаемость слов, составляя тем самым относительную частотную оценку $P(x_t|x_{t-1})$, а наиболее подходящая последовательность подсчитывается с помощью динамического программирования.
### The Order of Decoding
- слева-направо. Почему?: потому что мы не арабы и не евреи (и даже не пишем на персидском, синдхи и урду) и это интуитивно правильнее, что для нас, что для моделей.
- предсказательная способность соседних слов (и других токенов тоже) гораздо лучше, чем произвольно взятых. А изменение текущего состояния никогда не изменит прошлого, но может поменять будущее, то есть течение времени рассматривается слева направо, что есть более простая задача. Для дополнительного чтения есть [Peters et al., 2017a](https://web.math.ku.dk/~peters/jonas_files/ElementsOfCausalInference.pdf)